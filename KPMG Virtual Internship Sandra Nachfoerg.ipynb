{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transactions= pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name= \"Transactions\")\n",
    "NewCustomerList= pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name= \"NewCustomerList\")\n",
    "CustomerDemographic= pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name= \"CustomerDemographic\")\n",
    "CustomerAddress= pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name= \"CustomerAddress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_header(df):\n",
    "    new_header= df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df.columns= new_header\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>online_order</th>\n",
       "      <th>order_status</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_line</th>\n",
       "      <th>product_class</th>\n",
       "      <th>product_size</th>\n",
       "      <th>list_price</th>\n",
       "      <th>standard_cost</th>\n",
       "      <th>product_first_sold_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2950</td>\n",
       "      <td>2017-02-25 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Solex</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>71.49</td>\n",
       "      <td>53.62</td>\n",
       "      <td>41245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3120</td>\n",
       "      <td>2017-05-21 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Trek Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>large</td>\n",
       "      <td>2091.47</td>\n",
       "      <td>388.92</td>\n",
       "      <td>41701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>402</td>\n",
       "      <td>2017-10-16 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>OHM Cycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>1793.43</td>\n",
       "      <td>248.82</td>\n",
       "      <td>36361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>3135</td>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Norco Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>1198.46</td>\n",
       "      <td>381.1</td>\n",
       "      <td>36145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>787</td>\n",
       "      <td>2017-10-01 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Giant Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>large</td>\n",
       "      <td>1765.3</td>\n",
       "      <td>709.48</td>\n",
       "      <td>42226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 transaction_id product_id customer_id     transaction_date online_order  \\\n",
       "1              1          2        2950  2017-02-25 00:00:00        False   \n",
       "2              2          3        3120  2017-05-21 00:00:00         True   \n",
       "3              3         37         402  2017-10-16 00:00:00        False   \n",
       "4              4         88        3135  2017-08-31 00:00:00        False   \n",
       "5              5         78         787  2017-10-01 00:00:00         True   \n",
       "\n",
       "0 order_status           brand product_line product_class product_size  \\\n",
       "1     Approved           Solex     Standard        medium       medium   \n",
       "2     Approved   Trek Bicycles     Standard        medium        large   \n",
       "3     Approved      OHM Cycles     Standard           low       medium   \n",
       "4     Approved  Norco Bicycles     Standard        medium       medium   \n",
       "5     Approved  Giant Bicycles     Standard        medium        large   \n",
       "\n",
       "0 list_price standard_cost product_first_sold_date  \n",
       "1      71.49         53.62                   41245  \n",
       "2    2091.47        388.92                   41701  \n",
       "3    1793.43        248.82                   36361  \n",
       "4    1198.46         381.1                   36145  \n",
       "5     1765.3        709.48                   42226  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transactions= cleanup_header(Transactions)\n",
    "Transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create a column for profit and frequency of purchase per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_Trans(df):\n",
    "    df[\"profit\"]= df['list_price']- df[\"standard_cost\"]\n",
    "    df['Frequency']= df.customer_id.value_counts()\n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>online_order</th>\n",
       "      <th>order_status</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_line</th>\n",
       "      <th>product_class</th>\n",
       "      <th>product_size</th>\n",
       "      <th>list_price</th>\n",
       "      <th>standard_cost</th>\n",
       "      <th>product_first_sold_date</th>\n",
       "      <th>profit</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2950</td>\n",
       "      <td>2017-02-25 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Solex</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>71.49</td>\n",
       "      <td>53.62</td>\n",
       "      <td>41245</td>\n",
       "      <td>17.87</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3120</td>\n",
       "      <td>2017-05-21 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Trek Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>large</td>\n",
       "      <td>2091.47</td>\n",
       "      <td>388.92</td>\n",
       "      <td>41701</td>\n",
       "      <td>1702.55</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>402</td>\n",
       "      <td>2017-10-16 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>OHM Cycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>1793.43</td>\n",
       "      <td>248.82</td>\n",
       "      <td>36361</td>\n",
       "      <td>1544.61</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>3135</td>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Norco Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>1198.46</td>\n",
       "      <td>381.1</td>\n",
       "      <td>36145</td>\n",
       "      <td>817.36</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>787</td>\n",
       "      <td>2017-10-01 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Giant Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>large</td>\n",
       "      <td>1765.3</td>\n",
       "      <td>709.48</td>\n",
       "      <td>42226</td>\n",
       "      <td>1055.82</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 transaction_id product_id customer_id     transaction_date online_order  \\\n",
       "1              1          2        2950  2017-02-25 00:00:00        False   \n",
       "2              2          3        3120  2017-05-21 00:00:00         True   \n",
       "3              3         37         402  2017-10-16 00:00:00        False   \n",
       "4              4         88        3135  2017-08-31 00:00:00        False   \n",
       "5              5         78         787  2017-10-01 00:00:00         True   \n",
       "\n",
       "0 order_status           brand product_line product_class product_size  \\\n",
       "1     Approved           Solex     Standard        medium       medium   \n",
       "2     Approved   Trek Bicycles     Standard        medium        large   \n",
       "3     Approved      OHM Cycles     Standard           low       medium   \n",
       "4     Approved  Norco Bicycles     Standard        medium       medium   \n",
       "5     Approved  Giant Bicycles     Standard        medium        large   \n",
       "\n",
       "0 list_price standard_cost product_first_sold_date   profit  Frequency  \n",
       "1      71.49         53.62                   41245    17.87       11.0  \n",
       "2    2091.47        388.92                   41701  1702.55        3.0  \n",
       "3    1793.43        248.82                   36361  1544.61        8.0  \n",
       "4    1198.46         381.1                   36145   817.36        2.0  \n",
       "5     1765.3        709.48                   42226  1055.82        6.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transactions= fix_Trans(Transactions)\n",
    "Transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Keep only the columns you really need: In this case customer_id, transaction_date, profit, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransactionsNew= Transactions[[\"customer_id\", \"transaction_date\", \"profit\", \"Frequency\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_NewTrans(df):\n",
    "    #TransactionsNew= df[[\"customer_id\", \"transaction_date\", \"profit\", \"Frequency\"]]\n",
    "    TransactionsNew['transaction_date'].max()\n",
    "\n",
    "    import datetime as dt\n",
    "\n",
    "    NOW = dt.datetime(2017,12,31)\n",
    "    TransactionsNew['Date'] = pd.to_datetime(TransactionsNew['transaction_date'])\n",
    "    TransactionsNew.drop(columns = [\"transaction_date\"], inplace= True)\n",
    "    \n",
    "    return (TransactionsNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>profit</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2950</td>\n",
       "      <td>17.87</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2017-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3120</td>\n",
       "      <td>1702.55</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>402</td>\n",
       "      <td>1544.61</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3135</td>\n",
       "      <td>817.36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>787</td>\n",
       "      <td>1055.82</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2017-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 customer_id   profit  Frequency       Date\n",
       "1        2950    17.87       11.0 2017-02-25\n",
       "2        3120  1702.55        3.0 2017-05-21\n",
       "3         402  1544.61        8.0 2017-10-16\n",
       "4        3135   817.36        2.0 2017-08-31\n",
       "5         787  1055.82        6.0 2017-10-01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TransactionsNew.drop(columns = [\"transaction_date\"], inplace= True)\n",
    "TransactionsNew= prepare_NewTrans(TransactionsNew)\n",
    "TransactionsNew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a RFM Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_table(df):\n",
    "    import datetime as dt\n",
    "    NOW = dt.datetime(2017,12,31)\n",
    "    \n",
    "    rfmTable = df.groupby('customer_id').agg({'Date': lambda x: (NOW - x.max()).days, \n",
    "                                                           'Frequency': lambda x: len(x), \n",
    "                                                           'profit': lambda x: x.sum()})\n",
    "    rfmTable.rename(columns = {'Date': 'Recency', 'profit': 'MonetaryValue'}, inplace = True)\n",
    "\n",
    "    return (rfmTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>MonetaryValue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3018.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2226.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3362.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>2.0</td>\n",
       "      <td>220.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2394.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Recency  Frequency  MonetaryValue\n",
       "customer_id                                   \n",
       "1                  8       11.0        3018.09\n",
       "2                129        3.0        2226.26\n",
       "3                103        8.0        3362.81\n",
       "4                196        2.0         220.57\n",
       "5                 17        6.0        2394.94"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm_Table = rfm_table(TransactionsNew)\n",
    "rfm_Table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>online_order</th>\n",
       "      <th>order_status</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_line</th>\n",
       "      <th>product_class</th>\n",
       "      <th>product_size</th>\n",
       "      <th>list_price</th>\n",
       "      <th>standard_cost</th>\n",
       "      <th>product_first_sold_date</th>\n",
       "      <th>profit</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-12-23 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>OHM Cycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>235.63</td>\n",
       "      <td>125.07</td>\n",
       "      <td>38482</td>\n",
       "      <td>110.56</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3765</th>\n",
       "      <td>3765</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-06 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Solex</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>1577.53</td>\n",
       "      <td>826.51</td>\n",
       "      <td>39526</td>\n",
       "      <td>751.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>5157</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-11 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Trek Bicycles</td>\n",
       "      <td>Road</td>\n",
       "      <td>low</td>\n",
       "      <td>small</td>\n",
       "      <td>1720.7</td>\n",
       "      <td>1531.42</td>\n",
       "      <td>37823</td>\n",
       "      <td>189.28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9785</th>\n",
       "      <td>9785</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-05 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Norco Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>360.4</td>\n",
       "      <td>270.3</td>\n",
       "      <td>37873</td>\n",
       "      <td>90.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13424</th>\n",
       "      <td>13424</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-21 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Solex</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>71.49</td>\n",
       "      <td>53.62</td>\n",
       "      <td>38573</td>\n",
       "      <td>17.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13644</th>\n",
       "      <td>13644</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-19 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Giant Bicycles</td>\n",
       "      <td>Road</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>1538.99</td>\n",
       "      <td>829.65</td>\n",
       "      <td>33552</td>\n",
       "      <td>709.34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14486</th>\n",
       "      <td>14486</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-27 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Norco Bicycles</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>low</td>\n",
       "      <td>small</td>\n",
       "      <td>688.63</td>\n",
       "      <td>612.88</td>\n",
       "      <td>33455</td>\n",
       "      <td>75.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14931</th>\n",
       "      <td>14931</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-12-14 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Giant Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>230.91</td>\n",
       "      <td>173.18</td>\n",
       "      <td>37337</td>\n",
       "      <td>57.73</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15663</th>\n",
       "      <td>15663</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-04 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Giant Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>642.7</td>\n",
       "      <td>211.37</td>\n",
       "      <td>40618</td>\n",
       "      <td>431.33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16423</th>\n",
       "      <td>16423</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-12-09 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>OHM Cycles</td>\n",
       "      <td>Road</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>742.54</td>\n",
       "      <td>667.4</td>\n",
       "      <td>37838</td>\n",
       "      <td>75.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18970</th>\n",
       "      <td>18970</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-29 00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Giant Bicycles</td>\n",
       "      <td>Standard</td>\n",
       "      <td>high</td>\n",
       "      <td>medium</td>\n",
       "      <td>1274.93</td>\n",
       "      <td>764.96</td>\n",
       "      <td>35378</td>\n",
       "      <td>509.97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0     transaction_id product_id customer_id     transaction_date online_order  \\\n",
       "94                94         86           1  2017-12-23 00:00:00        False   \n",
       "3765            3765         38           1  2017-04-06 00:00:00         True   \n",
       "5157            5157         47           1  2017-05-11 00:00:00         True   \n",
       "9785            9785         72           1  2017-01-05 00:00:00        False   \n",
       "13424          13424          2           1  2017-02-21 00:00:00        False   \n",
       "13644          13644         25           1  2017-05-19 00:00:00        False   \n",
       "14486          14486         23           1  2017-03-27 00:00:00        False   \n",
       "14931          14931         31           1  2017-12-14 00:00:00         True   \n",
       "15663          15663         32           1  2017-06-04 00:00:00         True   \n",
       "16423          16423          9           1  2017-12-09 00:00:00         True   \n",
       "18970          18970         11           1  2017-03-29 00:00:00         True   \n",
       "\n",
       "0     order_status           brand product_line product_class product_size  \\\n",
       "94        Approved      OHM Cycles     Standard        medium       medium   \n",
       "3765      Approved           Solex     Standard        medium       medium   \n",
       "5157      Approved   Trek Bicycles         Road           low        small   \n",
       "9785      Approved  Norco Bicycles     Standard        medium       medium   \n",
       "13424     Approved           Solex     Standard        medium       medium   \n",
       "13644     Approved  Giant Bicycles         Road        medium       medium   \n",
       "14486     Approved  Norco Bicycles     Mountain           low        small   \n",
       "14931     Approved  Giant Bicycles     Standard        medium       medium   \n",
       "15663     Approved  Giant Bicycles     Standard        medium       medium   \n",
       "16423     Approved      OHM Cycles         Road        medium       medium   \n",
       "18970     Approved  Giant Bicycles     Standard          high       medium   \n",
       "\n",
       "0     list_price standard_cost product_first_sold_date  profit  Frequency  \n",
       "94        235.63        125.07                   38482  110.56        7.0  \n",
       "3765     1577.53        826.51                   39526  751.02        NaN  \n",
       "5157      1720.7       1531.42                   37823  189.28        NaN  \n",
       "9785       360.4         270.3                   37873    90.1        NaN  \n",
       "13424      71.49         53.62                   38573   17.87        NaN  \n",
       "13644    1538.99        829.65                   33552  709.34        NaN  \n",
       "14486     688.63        612.88                   33455   75.75        NaN  \n",
       "14931     230.91        173.18                   37337   57.73        NaN  \n",
       "15663      642.7        211.37                   40618  431.33        NaN  \n",
       "16423     742.54         667.4                   37838   75.14        NaN  \n",
       "18970    1274.93        764.96                   35378  509.97        NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transactions[Transactions[\"customer_id\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Create a Score for each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = \"Recency\"\n",
    "# d[p]= quantiles\n",
    "# Define two seperate functions\n",
    "\n",
    "quantiles = rfm_Table.quantile(q=[0.25,0.5,0.75])\n",
    "quantiles = quantiles.to_dict()\n",
    "\n",
    "def RScore(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 1\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 2\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "def FMScore(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 4\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 3\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_quartiles(df):\n",
    "    df['r_quartile'] = df['Recency'].apply(RScore, args=('Recency',quantiles,))\n",
    "    df['f_quartile'] = df['Frequency'].apply(FMScore, args=('Frequency',quantiles,))\n",
    "    df['m_quartile'] = df['MonetaryValue'].apply(FMScore, args=('MonetaryValue',quantiles,))\n",
    "\n",
    "    df['RFMScore'] = df.r_quartile.map(str) + df.f_quartile.map(str) + df.m_quartile.map(str)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmTable= rfm_quartiles(rfm_Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>MonetaryValue</th>\n",
       "      <th>r_quartile</th>\n",
       "      <th>f_quartile</th>\n",
       "      <th>m_quartile</th>\n",
       "      <th>RFMScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3018.09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2226.26</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3362.81</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>2.0</td>\n",
       "      <td>220.57</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2394.94</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Recency  Frequency  MonetaryValue  r_quartile  f_quartile  \\\n",
       "customer_id                                                              \n",
       "1                  8       11.0        3018.09           1           1   \n",
       "2                129        3.0        2226.26           4           4   \n",
       "3                103        8.0        3362.81           4           1   \n",
       "4                196        2.0         220.57           4           4   \n",
       "5                 17        6.0        2394.94           1           3   \n",
       "\n",
       "             m_quartile RFMScore  \n",
       "customer_id                       \n",
       "1                     2      112  \n",
       "2                     3      443  \n",
       "3                     2      412  \n",
       "4                     4      444  \n",
       "5                     3      133  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Checking who are our number 1 customers based on Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>MonetaryValue</th>\n",
       "      <th>r_quartile</th>\n",
       "      <th>f_quartile</th>\n",
       "      <th>m_quartile</th>\n",
       "      <th>RFMScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5925.74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4899.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6052.93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6435.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5665.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5563.63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6874.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8143.93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5383.48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6710.06</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Recency  Frequency  MonetaryValue  r_quartile  f_quartile  \\\n",
       "customer_id                                                              \n",
       "3433               1       10.0        5925.74           1           1   \n",
       "934                1        8.0        4899.60           1           1   \n",
       "2165               1       10.0        6052.93           1           1   \n",
       "1506               1       10.0        6435.10           1           1   \n",
       "151                1        8.0        5665.41           1           1   \n",
       "2055               1        9.0        5563.63           1           1   \n",
       "2434               1        9.0        6874.53           1           1   \n",
       "2659               1       12.0        8143.93           1           1   \n",
       "1000               1        9.0        5383.48           1           1   \n",
       "983                2        8.0        6710.06           1           1   \n",
       "\n",
       "             m_quartile RFMScore  \n",
       "customer_id                       \n",
       "3433                  1      111  \n",
       "934                   1      111  \n",
       "2165                  1      111  \n",
       "1506                  1      111  \n",
       "151                   1      111  \n",
       "2055                  1      111  \n",
       "2434                  1      111  \n",
       "2659                  1      111  \n",
       "1000                  1      111  \n",
       "983                   1      111  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmTable[rfmTable['RFMScore']=='111'].sort_values('Recency', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Merging the RFM Table with our Customer Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>past_3_years_bike_related_purchases</th>\n",
       "      <th>DOB</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_industry_category</th>\n",
       "      <th>wealth_segment</th>\n",
       "      <th>deceased_indicator</th>\n",
       "      <th>default</th>\n",
       "      <th>owns_car</th>\n",
       "      <th>tenure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Laraine</td>\n",
       "      <td>Medendorp</td>\n",
       "      <td>F</td>\n",
       "      <td>93</td>\n",
       "      <td>1953-10-12 00:00:00</td>\n",
       "      <td>Executive Secretary</td>\n",
       "      <td>Health</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>\"'</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Eli</td>\n",
       "      <td>Bockman</td>\n",
       "      <td>Male</td>\n",
       "      <td>81</td>\n",
       "      <td>1980-12-16 00:00:00</td>\n",
       "      <td>Administrative Officer</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;script&gt;alert('hi')&lt;/script&gt;</td>\n",
       "      <td>Yes</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Arlin</td>\n",
       "      <td>Dearle</td>\n",
       "      <td>Male</td>\n",
       "      <td>61</td>\n",
       "      <td>1954-01-20 00:00:00</td>\n",
       "      <td>Recruiting Manager</td>\n",
       "      <td>Property</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>2018-02-01 00:00:00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Talbot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>1961-10-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>() { _; } &gt;_[$($())] { touch /tmp/blns.shellsh...</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Sheila-kathryn</td>\n",
       "      <td>Calton</td>\n",
       "      <td>Female</td>\n",
       "      <td>56</td>\n",
       "      <td>1977-05-13 00:00:00</td>\n",
       "      <td>Senior Editor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Affluent Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>NIL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 customer_id      first_name  last_name  gender  \\\n",
       "1           1         Laraine  Medendorp       F   \n",
       "2           2             Eli    Bockman    Male   \n",
       "3           3           Arlin     Dearle    Male   \n",
       "4           4          Talbot        NaN    Male   \n",
       "5           5  Sheila-kathryn     Calton  Female   \n",
       "\n",
       "0 past_3_years_bike_related_purchases                  DOB  \\\n",
       "1                                  93  1953-10-12 00:00:00   \n",
       "2                                  81  1980-12-16 00:00:00   \n",
       "3                                  61  1954-01-20 00:00:00   \n",
       "4                                  33  1961-10-03 00:00:00   \n",
       "5                                  56  1977-05-13 00:00:00   \n",
       "\n",
       "0               job_title job_industry_category     wealth_segment  \\\n",
       "1     Executive Secretary                Health      Mass Customer   \n",
       "2  Administrative Officer    Financial Services      Mass Customer   \n",
       "3      Recruiting Manager              Property      Mass Customer   \n",
       "4                     NaN                    IT      Mass Customer   \n",
       "5           Senior Editor                   NaN  Affluent Customer   \n",
       "\n",
       "0 deceased_indicator                                            default  \\\n",
       "1                  N                                                 \"'   \n",
       "2                  N                       <script>alert('hi')</script>   \n",
       "3                  N                                2018-02-01 00:00:00   \n",
       "4                  N  () { _; } >_[$($())] { touch /tmp/blns.shellsh...   \n",
       "5                  N                                                NIL   \n",
       "\n",
       "0 owns_car tenure  \n",
       "1      Yes     11  \n",
       "2      Yes     16  \n",
       "3      Yes     15  \n",
       "4       No      7  \n",
       "5      Yes      8  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Customer_Demographic= cleanup_header(CustomerDemographic)\n",
    "Customer_Address = cleanup_header(CustomerAddress)\n",
    "Customer_Demographic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_customers(Customer_Demographic, Customer_Address):\n",
    "    Customer_Demographic.set_index(\"customer_id\", inplace=True)\n",
    "    Customer_Address.set_index(\"customer_id\", inplace=True)\n",
    "    return Customer_Demographic.join(Customer_Address, on=\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>past_3_years_bike_related_purchases</th>\n",
       "      <th>DOB</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_industry_category</th>\n",
       "      <th>wealth_segment</th>\n",
       "      <th>deceased_indicator</th>\n",
       "      <th>default</th>\n",
       "      <th>owns_car</th>\n",
       "      <th>tenure</th>\n",
       "      <th>address</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>property_valuation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laraine</td>\n",
       "      <td>Medendorp</td>\n",
       "      <td>F</td>\n",
       "      <td>93</td>\n",
       "      <td>1953-10-12 00:00:00</td>\n",
       "      <td>Executive Secretary</td>\n",
       "      <td>Health</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>\"'</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>060 Morning Avenue</td>\n",
       "      <td>2016</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eli</td>\n",
       "      <td>Bockman</td>\n",
       "      <td>Male</td>\n",
       "      <td>81</td>\n",
       "      <td>1980-12-16 00:00:00</td>\n",
       "      <td>Administrative Officer</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;script&gt;alert('hi')&lt;/script&gt;</td>\n",
       "      <td>Yes</td>\n",
       "      <td>16</td>\n",
       "      <td>6 Meadow Vale Court</td>\n",
       "      <td>2153</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arlin</td>\n",
       "      <td>Dearle</td>\n",
       "      <td>Male</td>\n",
       "      <td>61</td>\n",
       "      <td>1954-01-20 00:00:00</td>\n",
       "      <td>Recruiting Manager</td>\n",
       "      <td>Property</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>2018-02-01 00:00:00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Talbot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>1961-10-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>() { _; } &gt;_[$($())] { touch /tmp/blns.shellsh...</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>0 Holy Cross Court</td>\n",
       "      <td>4211</td>\n",
       "      <td>QLD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sheila-kathryn</td>\n",
       "      <td>Calton</td>\n",
       "      <td>Female</td>\n",
       "      <td>56</td>\n",
       "      <td>1977-05-13 00:00:00</td>\n",
       "      <td>Senior Editor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Affluent Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>NIL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>17979 Del Mar Point</td>\n",
       "      <td>2448</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                first_name  last_name  gender  \\\n",
       "customer_id                                      \n",
       "1                   Laraine  Medendorp       F   \n",
       "2                       Eli    Bockman    Male   \n",
       "3                     Arlin     Dearle    Male   \n",
       "4                    Talbot        NaN    Male   \n",
       "5            Sheila-kathryn     Calton  Female   \n",
       "\n",
       "0           past_3_years_bike_related_purchases                  DOB  \\\n",
       "customer_id                                                            \n",
       "1                                            93  1953-10-12 00:00:00   \n",
       "2                                            81  1980-12-16 00:00:00   \n",
       "3                                            61  1954-01-20 00:00:00   \n",
       "4                                            33  1961-10-03 00:00:00   \n",
       "5                                            56  1977-05-13 00:00:00   \n",
       "\n",
       "0                         job_title job_industry_category     wealth_segment  \\\n",
       "customer_id                                                                    \n",
       "1               Executive Secretary                Health      Mass Customer   \n",
       "2            Administrative Officer    Financial Services      Mass Customer   \n",
       "3                Recruiting Manager              Property      Mass Customer   \n",
       "4                               NaN                    IT      Mass Customer   \n",
       "5                     Senior Editor                   NaN  Affluent Customer   \n",
       "\n",
       "0           deceased_indicator  \\\n",
       "customer_id                      \n",
       "1                            N   \n",
       "2                            N   \n",
       "3                            N   \n",
       "4                            N   \n",
       "5                            N   \n",
       "\n",
       "0                                                      default owns_car  \\\n",
       "customer_id                                                               \n",
       "1                                                           \"'      Yes   \n",
       "2                                 <script>alert('hi')</script>      Yes   \n",
       "3                                          2018-02-01 00:00:00      Yes   \n",
       "4            () { _; } >_[$($())] { touch /tmp/blns.shellsh...       No   \n",
       "5                                                          NIL      Yes   \n",
       "\n",
       "0           tenure              address postcode            state    country  \\\n",
       "customer_id                                                                    \n",
       "1               11   060 Morning Avenue     2016  New South Wales  Australia   \n",
       "2               16  6 Meadow Vale Court     2153  New South Wales  Australia   \n",
       "3               15                  NaN      NaN              NaN        NaN   \n",
       "4                7   0 Holy Cross Court     4211              QLD  Australia   \n",
       "5                8  17979 Del Mar Point     2448  New South Wales  Australia   \n",
       "\n",
       "0           property_valuation  \n",
       "customer_id                     \n",
       "1                           10  \n",
       "2                           10  \n",
       "3                          NaN  \n",
       "4                            9  \n",
       "5                            4  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Customers = merge_customers(Customer_Demographic, Customer_Address)\n",
    "Customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomersRFM= Customers.join(rfmTable, on=\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>past_3_years_bike_related_purchases</th>\n",
       "      <th>DOB</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_industry_category</th>\n",
       "      <th>wealth_segment</th>\n",
       "      <th>deceased_indicator</th>\n",
       "      <th>default</th>\n",
       "      <th>...</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>property_valuation</th>\n",
       "      <th>Recency</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>MonetaryValue</th>\n",
       "      <th>r_quartile</th>\n",
       "      <th>f_quartile</th>\n",
       "      <th>m_quartile</th>\n",
       "      <th>RFMScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laraine</td>\n",
       "      <td>Medendorp</td>\n",
       "      <td>F</td>\n",
       "      <td>93</td>\n",
       "      <td>1953-10-12 00:00:00</td>\n",
       "      <td>Executive Secretary</td>\n",
       "      <td>Health</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>\"'</td>\n",
       "      <td>...</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3018.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eli</td>\n",
       "      <td>Bockman</td>\n",
       "      <td>Male</td>\n",
       "      <td>81</td>\n",
       "      <td>1980-12-16 00:00:00</td>\n",
       "      <td>Administrative Officer</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;script&gt;alert('hi')&lt;/script&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>129.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2226.26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arlin</td>\n",
       "      <td>Dearle</td>\n",
       "      <td>Male</td>\n",
       "      <td>61</td>\n",
       "      <td>1954-01-20 00:00:00</td>\n",
       "      <td>Recruiting Manager</td>\n",
       "      <td>Property</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>2018-02-01 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3362.81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Talbot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>1961-10-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT</td>\n",
       "      <td>Mass Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>() { _; } &gt;_[$($())] { touch /tmp/blns.shellsh...</td>\n",
       "      <td>...</td>\n",
       "      <td>QLD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>9</td>\n",
       "      <td>196.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>220.57</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sheila-kathryn</td>\n",
       "      <td>Calton</td>\n",
       "      <td>Female</td>\n",
       "      <td>56</td>\n",
       "      <td>1977-05-13 00:00:00</td>\n",
       "      <td>Senior Editor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Affluent Customer</td>\n",
       "      <td>N</td>\n",
       "      <td>NIL</td>\n",
       "      <td>...</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2394.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 first_name  last_name  gender  \\\n",
       "customer_id                                      \n",
       "1                   Laraine  Medendorp       F   \n",
       "2                       Eli    Bockman    Male   \n",
       "3                     Arlin     Dearle    Male   \n",
       "4                    Talbot        NaN    Male   \n",
       "5            Sheila-kathryn     Calton  Female   \n",
       "\n",
       "            past_3_years_bike_related_purchases                  DOB  \\\n",
       "customer_id                                                            \n",
       "1                                            93  1953-10-12 00:00:00   \n",
       "2                                            81  1980-12-16 00:00:00   \n",
       "3                                            61  1954-01-20 00:00:00   \n",
       "4                                            33  1961-10-03 00:00:00   \n",
       "5                                            56  1977-05-13 00:00:00   \n",
       "\n",
       "                          job_title job_industry_category     wealth_segment  \\\n",
       "customer_id                                                                    \n",
       "1               Executive Secretary                Health      Mass Customer   \n",
       "2            Administrative Officer    Financial Services      Mass Customer   \n",
       "3                Recruiting Manager              Property      Mass Customer   \n",
       "4                               NaN                    IT      Mass Customer   \n",
       "5                     Senior Editor                   NaN  Affluent Customer   \n",
       "\n",
       "            deceased_indicator  \\\n",
       "customer_id                      \n",
       "1                            N   \n",
       "2                            N   \n",
       "3                            N   \n",
       "4                            N   \n",
       "5                            N   \n",
       "\n",
       "                                                       default  ...  \\\n",
       "customer_id                                                     ...   \n",
       "1                                                           \"'  ...   \n",
       "2                                 <script>alert('hi')</script>  ...   \n",
       "3                                          2018-02-01 00:00:00  ...   \n",
       "4            () { _; } >_[$($())] { touch /tmp/blns.shellsh...  ...   \n",
       "5                                                          NIL  ...   \n",
       "\n",
       "                       state    country property_valuation Recency Frequency  \\\n",
       "customer_id                                                                    \n",
       "1            New South Wales  Australia                 10     8.0      11.0   \n",
       "2            New South Wales  Australia                 10   129.0       3.0   \n",
       "3                        NaN        NaN                NaN   103.0       8.0   \n",
       "4                        QLD  Australia                  9   196.0       2.0   \n",
       "5            New South Wales  Australia                  4    17.0       6.0   \n",
       "\n",
       "            MonetaryValue r_quartile  f_quartile  m_quartile  RFMScore  \n",
       "customer_id                                                             \n",
       "1                 3018.09        1.0         1.0         2.0       112  \n",
       "2                 2226.26        4.0         4.0         3.0       443  \n",
       "3                 3362.81        4.0         1.0         2.0       412  \n",
       "4                  220.57        4.0         4.0         4.0       444  \n",
       "5                 2394.94        1.0         3.0         3.0       133  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustomersRFM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_customer(df):\n",
    "    df.drop(columns=[\"first_name\", \"last_name\", \"job_title\", \n",
    "                     \"deceased_indicator\", \"address\", \"postcode\", \"country\"], inplace=True)\n",
    "    # Gender\n",
    "    genderDict = {\"F\": \"Female\", \"M\": \"Male\", \"Femal\": \"Female\", \"U\": \"Undefined\"}\n",
    "    df['gender'].replace(genderDict, inplace= True)\n",
    "    \n",
    "    # DOB\n",
    "    now = datetime.datetime.now()\n",
    "    df['age']= now.year - pd.DatetimeIndex(df['DOB']).year\n",
    "    df.drop(columns=[\"DOB\"], inplace=True)\n",
    "    \n",
    "    # Job industry category\n",
    "    df[\"job_industry_category\"].fillna(\"n/a\", inplace=True)\n",
    "     \n",
    "    # tenure\n",
    "    df[\"tenure\"].fillna(df[\"tenure\"].mean(), inplace=True)\n",
    "    \n",
    "    # state\n",
    "    stateDict = {\"New South Wales\": \"NSW\", \"Victoria\": \"VIC\", \"Queensland\": \"QLD\"}\n",
    "    df['state'].replace(stateDict, inplace= True)\n",
    "    df[\"state\"].fillna(\"n/a\", inplace=True)\n",
    "    \n",
    "    # property valuation\n",
    "    df[\"property_valuation\"] = df[\"property_valuation\"].astype(\"float64\")\n",
    "    df[\"property_valuation\"].fillna(df[\"property_valuation\"].mean(), inplace=True)\n",
    "    \n",
    "    # age\n",
    "    df[\"age\"].fillna(df[\"age\"].mean(), inplace=True)\n",
    "    \n",
    "    # Or rename the existing DataFrame (rather than creating a copy) \n",
    "    df.rename(columns={'past_3_years_bike_related_purchases': '3y_bike'}, inplace=True)\n",
    "    df[\"3y_bike\"] = df[\"3y_bike\"].astype(\"float64\")\n",
    "    \n",
    "    df.drop(columns = ['default'], inplace = True)\n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CustomersRFM = cleanup_customer(CustomersRFM)\n",
    "CustomersRFM.to_csv(\"CustomersRFM.csv\")\n",
    "\n",
    "# x= CustomerWithRMF[\"3y_bike\"]\n",
    "# y = CustomerWithRMF[\"RFMScore\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Grouping to find out some trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3y_bike  MonetaryValue\n",
       "0.0      6433.75          1\n",
       "1.0      5020.67          1\n",
       "2.0      4809.95          1\n",
       "5.0      4841.71          1\n",
       "         5580.57          1\n",
       "                         ..\n",
       "97.0     4557.65          1\n",
       "         4778.33          1\n",
       "         6692.39          1\n",
       "98.0     4790.82          1\n",
       "99.0     4534.24          1\n",
       "Name: MonetaryValue, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recency = CustomersRFM[CustomersRFM['RFMScore']=='111'].sort_values('Recency', ascending=True).head(100)\n",
    "Recency.groupby(\"3y_bike\").MonetaryValue.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax1 = CustomersRFM.plot.scatter(x='Frequency',\n",
    "                      y='age',\n",
    "                      c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conclusion: Mass Customers purchase more often\n",
    "## Company can aim at driving more monetary value by aiming at customers in NSW as they tend to spend slightly more than\n",
    "#than other states\n",
    "##Companies can drive more frequent purchases by aiming at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_category_to_idx(df, col):   \n",
    "    l = df[col].unique()\n",
    "    d = {k: v for v, k in enumerate(l)}\n",
    "    df[col].replace(d, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_customer(df):\n",
    "    df.drop(columns=[\"first_name\", \"last_name\", \"job_title\", \n",
    "                     \"deceased_indicator\", \"address\", \"postcode\", \"country\"], inplace=True)\n",
    "    # Gender\n",
    "    genderDict = {\"F\": \"Female\", \"M\": \"Male\", \"Femal\": \"Female\", \"U\": \"Undefined\"}\n",
    "    df['gender'].replace(genderDict, inplace= True)\n",
    "    col_category_to_idx(df, \"gender\")\n",
    "    \n",
    "    # DOB\n",
    "    now = datetime.datetime.now()\n",
    "    df['age']= now.year - pd.DatetimeIndex(df['DOB']).year\n",
    "    df.drop(columns=[\"DOB\"], inplace=True)\n",
    "    \n",
    "    # Job industry category\n",
    "    df[\"job_industry_category\"].fillna(\"n/a\", inplace=True)\n",
    "    col_category_to_idx(df, \"job_industry_category\")\n",
    "    \n",
    "    col_category_to_idx(df, \"wealth_segment\")\n",
    "    col_category_to_idx(df, \"owns_car\")  \n",
    "    \n",
    "    # tenure\n",
    "    df[\"tenure\"].fillna(df[\"tenure\"].mean(), inplace=True)\n",
    "    \n",
    "    # state\n",
    "    stateDict = {\"New South Wales\": \"NSW\", \"Victoria\": \"VIC\", \"Queensland\": \"QLD\"}\n",
    "    df['state'].replace(stateDict, inplace= True)\n",
    "    df[\"state\"].fillna(\"n/a\", inplace=True)\n",
    "    col_category_to_idx(df, \"state\")  \n",
    "    \n",
    "    # property valuation\n",
    "    df[\"property_valuation\"] = df[\"property_valuation\"].astype(\"float64\")\n",
    "    df[\"property_valuation\"].fillna(df[\"property_valuation\"].mean(), inplace=True)\n",
    "    \n",
    "    # age\n",
    "    df[\"age\"].fillna(df[\"age\"].mean(), inplace=True)\n",
    "    \n",
    "    # Or rename the existing DataFrame (rather than creating a copy) \n",
    "    df.rename(columns={'past_3_years_bike_related_purchases': '3y_bike'}, inplace=True)\n",
    "    df[\"3y_bike\"] = df[\"3y_bike\"].astype(\"float64\")\n",
    "    \n",
    "    df.drop(columns = ['default'], inplace = True)\n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                     int64\n",
       "3y_bike                  float64\n",
       "job_industry_category      int64\n",
       "wealth_segment             int64\n",
       "owns_car                   int64\n",
       "tenure                   float64\n",
       "state                      int64\n",
       "property_valuation       float64\n",
       "Recency                  float64\n",
       "Frequency                float64\n",
       "MonetaryValue            float64\n",
       "r_quartile               float64\n",
       "f_quartile               float64\n",
       "m_quartile               float64\n",
       "RFMScore                 float64\n",
       "age                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RfmInclusive.drop(columns =[\"default\"], inplace = True)\n",
    "#Rfmtest.drop(columns = [\"default\"], inplace = True)\n",
    "Rfmtest['RFMScore']= Rfmtest['RFMScore'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_features(df):\n",
    "    # expand category data to one-hot vectors\n",
    "    return pd.get_dummies(df, columns=[\"gender\", \"job_industry_category\", \n",
    "                                       \"wealth_segment\", \"owns_car\", \"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RfmCustomers= customer_features(Rfmtest)\n",
    "RfmCustomers= RfmCustomers.dropna(how= 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "RfmCustomers= RfmCustomers[[c for c in RfmCustomers if c not in ['RFMScore']] \n",
    "       + ['RFMScore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rfmtest1.dropna(how= 'any', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Dropped.drop(columns = ['RFMScore', 'Recency', 'Frequency', 'MonetaryValue']).to_numpy()\n",
    "# print(x.shape)\n",
    "# print (y.shape)\n",
    "RfmCustomers= RfmCustomers.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_inputs_outputs(data):\n",
    "    x = np.delete(data, -1, axis=1)\n",
    "    y = data[:, -1].reshape((-1, 1))\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[93.        , 11.        , 10.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [81.        , 16.        , 10.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [61.        , 15.        ,  7.51401401, ...,  1.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [28.        ,  5.        ,  4.        , ...,  0.        ,\n",
       "          0.        ,  1.        ],\n",
       "        [29.        ,  7.        ,  9.        , ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [71.        , 17.        , 10.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]),\n",
       " array([[112.],\n",
       "        [443.],\n",
       "        [412.],\n",
       "        ...,\n",
       "        [432.],\n",
       "        [321.],\n",
       "        [434.]]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_inputs_outputs(RfmCustomers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (2794, 32)\n",
      "Y_train (2794, 1)\n",
      "X_dev (524, 32)\n",
      "Y_dev (524, 1)\n",
      "X_test (175, 32)\n",
      "Y_test (175, 1)\n"
     ]
    }
   ],
   "source": [
    "data = RfmCustomers\n",
    "data_train, data_dev, data_test = np.split(data, [int(0.8 * len(data)), int(0.95*len(data))])\n",
    "\n",
    "X_train, Y_train = to_inputs_outputs(data_train)\n",
    "X_dev, Y_dev = to_inputs_outputs(data_dev)\n",
    "X_test, Y_test = to_inputs_outputs(data_test)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"Y_train\", Y_train.shape)\n",
    "print(\"X_dev\", X_dev.shape)\n",
    "print(\"Y_dev\", Y_dev.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"Y_test\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='linear', input_shape=(32,)))\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.1), loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 52830.2578 - mae: 166.4318 - val_loss: 10675.9795 - val_mae: 87.6213\n",
      "Epoch 2/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38782.9844 - mae: 140.8876 - val_loss: 5554.8306 - val_mae: 56.5545\n",
      "Epoch 3/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36631.2812 - mae: 140.3757 - val_loss: 98416.8516 - val_mae: 282.1941\n",
      "Epoch 4/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38996.0898 - mae: 139.0214 - val_loss: 5271.2773 - val_mae: 57.4702\n",
      "Epoch 5/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37398.6211 - mae: 141.5672 - val_loss: 36846.7500 - val_mae: 172.9465\n",
      "Epoch 6/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38737.0586 - mae: 151.0594 - val_loss: 8864.7490 - val_mae: 77.9031\n",
      "Epoch 7/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36812.7734 - mae: 122.6520 - val_loss: 103925.0781 - val_mae: 287.2475\n",
      "Epoch 8/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 40494.4609 - mae: 142.7183 - val_loss: 6653.5283 - val_mae: 63.5512\n",
      "Epoch 9/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36886.3477 - mae: 129.9764 - val_loss: 4413.3633 - val_mae: 48.4110\n",
      "Epoch 10/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36087.0312 - mae: 128.6866 - val_loss: 4064.3894 - val_mae: 49.7355\n",
      "Epoch 11/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36350.7812 - mae: 129.9191 - val_loss: 147835.2969 - val_mae: 335.3185\n",
      "Epoch 12/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36612.3203 - mae: 140.9055 - val_loss: 3707.2708 - val_mae: 49.9309\n",
      "Epoch 13/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34218.3203 - mae: 132.9374 - val_loss: 114597.2188 - val_mae: 291.0866\n",
      "Epoch 14/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33881.2969 - mae: 128.7541 - val_loss: 164923.1719 - val_mae: 360.0282\n",
      "Epoch 15/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 39558.9883 - mae: 136.4428 - val_loss: 4242.1616 - val_mae: 50.2995\n",
      "Epoch 16/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 39105.0781 - mae: 130.2297 - val_loss: 3487.7290 - val_mae: 46.2586\n",
      "Epoch 17/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32881.8281 - mae: 116.2732 - val_loss: 11179.6074 - val_mae: 91.2083\n",
      "Epoch 18/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 37371.1953 - mae: 135.0921 - val_loss: 82135.2656 - val_mae: 253.4268\n",
      "Epoch 19/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37959.1562 - mae: 148.4565 - val_loss: 218911.1875 - val_mae: 413.0497\n",
      "Epoch 20/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32726.9941 - mae: 129.3633 - val_loss: 8388.5596 - val_mae: 76.8637\n",
      "Epoch 21/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36276.5234 - mae: 146.4256 - val_loss: 107447.6406 - val_mae: 285.7019\n",
      "Epoch 22/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36050.7930 - mae: 134.9756 - val_loss: 212853.4062 - val_mae: 401.2012\n",
      "Epoch 23/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 37191.2305 - mae: 137.7590 - val_loss: 4801.7251 - val_mae: 55.6565\n",
      "Epoch 24/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35933.8008 - mae: 138.0577 - val_loss: 98337.8828 - val_mae: 274.4265\n",
      "Epoch 25/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34735.8438 - mae: 135.7183 - val_loss: 10378.3818 - val_mae: 87.0810\n",
      "Epoch 26/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37456.1133 - mae: 140.3744 - val_loss: 114030.7500 - val_mae: 294.4343\n",
      "Epoch 27/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34792.1680 - mae: 130.6126 - val_loss: 17615.1953 - val_mae: 111.1922\n",
      "Epoch 28/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 39228.4531 - mae: 143.2438 - val_loss: 31702.1191 - val_mae: 156.7073\n",
      "Epoch 29/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36385.1758 - mae: 126.8533 - val_loss: 2826.4270 - val_mae: 41.2292\n",
      "Epoch 30/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33596.1484 - mae: 132.6143 - val_loss: 149488.1719 - val_mae: 337.8404\n",
      "Epoch 31/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36483.9453 - mae: 139.9234 - val_loss: 45835.7617 - val_mae: 191.1983\n",
      "Epoch 32/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37574.9961 - mae: 147.2737 - val_loss: 3037.1406 - val_mae: 42.7452\n",
      "Epoch 33/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33658.6250 - mae: 135.4339 - val_loss: 134599.3438 - val_mae: 318.9331\n",
      "Epoch 34/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37657.0117 - mae: 141.4305 - val_loss: 34111.2031 - val_mae: 162.9937\n",
      "Epoch 35/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36942.8789 - mae: 137.2988 - val_loss: 94366.8828 - val_mae: 271.0568\n",
      "Epoch 36/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31974.6738 - mae: 134.1720 - val_loss: 94982.6016 - val_mae: 269.5394\n",
      "Epoch 37/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33980.4766 - mae: 125.9387 - val_loss: 167496.1250 - val_mae: 359.5027\n",
      "Epoch 38/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37241.5703 - mae: 136.2086 - val_loss: 2377.3472 - val_mae: 37.5288\n",
      "Epoch 39/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35953.6367 - mae: 127.5045 - val_loss: 4168.6958 - val_mae: 52.1396\n",
      "Epoch 40/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33910.7344 - mae: 122.7376 - val_loss: 13032.2695 - val_mae: 95.6189\n",
      "Epoch 41/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36161.0547 - mae: 120.7486 - val_loss: 43915.7070 - val_mae: 184.7495\n",
      "Epoch 42/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36293.0547 - mae: 137.7995 - val_loss: 2463.1328 - val_mae: 40.4984\n",
      "Epoch 43/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35730.6016 - mae: 136.5561 - val_loss: 3278.9312 - val_mae: 43.5645\n",
      "Epoch 44/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36302.8281 - mae: 136.2625 - val_loss: 3446.9512 - val_mae: 47.0902\n",
      "Epoch 45/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34313.7188 - mae: 128.7614 - val_loss: 16092.1465 - val_mae: 107.3566\n",
      "Epoch 46/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35800.1875 - mae: 126.0375 - val_loss: 5136.8032 - val_mae: 58.5668\n",
      "Epoch 47/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36603.6836 - mae: 131.9218 - val_loss: 51387.4180 - val_mae: 199.6184\n",
      "Epoch 48/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34127.0273 - mae: 125.7400 - val_loss: 17988.4297 - val_mae: 118.2607\n",
      "Epoch 49/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36415.9961 - mae: 139.5294 - val_loss: 31985.5391 - val_mae: 156.5000\n",
      "Epoch 50/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37369.4766 - mae: 131.6389 - val_loss: 56797.5195 - val_mae: 211.6170\n",
      "Epoch 51/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34126.0703 - mae: 141.5241 - val_loss: 1870.9622 - val_mae: 34.3467\n",
      "Epoch 52/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31788.5703 - mae: 114.2499 - val_loss: 2163.4316 - val_mae: 38.4177\n",
      "Epoch 53/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34863.5039 - mae: 131.5258 - val_loss: 15378.9922 - val_mae: 109.3961\n",
      "Epoch 54/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35427.4141 - mae: 140.4958 - val_loss: 121349.1641 - val_mae: 307.6641\n",
      "Epoch 55/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 38594.6289 - mae: 150.5647 - val_loss: 9405.2549 - val_mae: 83.6901\n",
      "Epoch 56/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32303.0840 - mae: 124.2254 - val_loss: 12401.7959 - val_mae: 93.6000\n",
      "Epoch 57/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 40605.8281 - mae: 126.1375 - val_loss: 1840.4116 - val_mae: 33.4428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31356.3242 - mae: 119.4984 - val_loss: 106457.9531 - val_mae: 288.1605\n",
      "Epoch 59/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36269.2109 - mae: 132.2088 - val_loss: 6412.9937 - val_mae: 65.7219\n",
      "Epoch 60/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36023.5352 - mae: 125.0192 - val_loss: 2440.4236 - val_mae: 38.4903\n",
      "Epoch 61/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36170.9219 - mae: 143.1261 - val_loss: 1794.5992 - val_mae: 34.7228\n",
      "Epoch 62/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36459.2930 - mae: 138.3017 - val_loss: 55932.0312 - val_mae: 205.1007\n",
      "Epoch 63/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34655.3711 - mae: 141.9797 - val_loss: 2280.8867 - val_mae: 38.7445\n",
      "Epoch 64/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35027.3086 - mae: 135.0594 - val_loss: 5331.7988 - val_mae: 59.0326\n",
      "Epoch 65/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33862.1992 - mae: 122.7635 - val_loss: 21550.1699 - val_mae: 129.5398\n",
      "Epoch 66/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37421.9648 - mae: 136.2458 - val_loss: 71950.3438 - val_mae: 235.6210\n",
      "Epoch 67/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32940.1641 - mae: 120.3126 - val_loss: 54413.2070 - val_mae: 202.7078\n",
      "Epoch 68/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33003.4375 - mae: 137.7871 - val_loss: 5424.9375 - val_mae: 61.3029\n",
      "Epoch 69/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35966.2070 - mae: 128.0246 - val_loss: 1409.2795 - val_mae: 30.4566\n",
      "Epoch 70/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33761.3477 - mae: 116.2759 - val_loss: 1792.2300 - val_mae: 32.4734\n",
      "Epoch 71/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31169.4668 - mae: 96.9337 - val_loss: 102926.7812 - val_mae: 282.1395\n",
      "Epoch 72/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36870.2148 - mae: 111.0268 - val_loss: 77136.9531 - val_mae: 242.1811\n",
      "Epoch 73/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36306.4414 - mae: 128.4057 - val_loss: 2007.1880 - val_mae: 36.6397\n",
      "Epoch 74/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31889.8555 - mae: 114.9398 - val_loss: 3470.3633 - val_mae: 46.9919\n",
      "Epoch 75/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35868.7734 - mae: 135.5039 - val_loss: 6987.9497 - val_mae: 70.6688\n",
      "Epoch 76/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31665.5898 - mae: 115.2510 - val_loss: 59839.4766 - val_mae: 216.9323\n",
      "Epoch 77/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37015.5703 - mae: 130.7008 - val_loss: 49407.1016 - val_mae: 197.9811\n",
      "Epoch 78/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36227.2969 - mae: 125.6980 - val_loss: 2396.8857 - val_mae: 37.5851\n",
      "Epoch 79/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31107.2871 - mae: 125.1689 - val_loss: 8273.5107 - val_mae: 75.8149\n",
      "Epoch 80/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37087.2148 - mae: 136.2951 - val_loss: 1362.6763 - val_mae: 30.6666\n",
      "Epoch 81/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34292.7148 - mae: 130.8396 - val_loss: 17194.1543 - val_mae: 113.9315\n",
      "Epoch 82/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35409.8086 - mae: 139.1856 - val_loss: 1234.0182 - val_mae: 26.5165\n",
      "Epoch 83/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36424.9492 - mae: 129.2841 - val_loss: 1148.1450 - val_mae: 28.1016\n",
      "Epoch 84/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34358.2500 - mae: 131.0677 - val_loss: 1272.6630 - val_mae: 26.7278\n",
      "Epoch 85/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37202.7305 - mae: 135.2564 - val_loss: 4169.5737 - val_mae: 52.0130\n",
      "Epoch 86/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33826.4883 - mae: 121.9090 - val_loss: 3705.5911 - val_mae: 51.8119\n",
      "Epoch 87/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32726.2227 - mae: 131.5208 - val_loss: 1696.4006 - val_mae: 33.0411\n",
      "Epoch 88/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33769.7422 - mae: 133.7085 - val_loss: 249628.1719 - val_mae: 439.8793\n",
      "Epoch 89/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34870.7930 - mae: 123.5502 - val_loss: 62342.5352 - val_mae: 217.8597\n",
      "Epoch 90/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35937.6367 - mae: 152.9961 - val_loss: 95430.8516 - val_mae: 271.5840\n",
      "Epoch 91/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33485.3047 - mae: 136.6130 - val_loss: 24704.5078 - val_mae: 137.9349\n",
      "Epoch 92/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36259.5547 - mae: 124.6470 - val_loss: 310396.6250 - val_mae: 486.6304\n",
      "Epoch 93/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32681.6387 - mae: 96.6702 - val_loss: 1231.7418 - val_mae: 24.7437\n",
      "Epoch 94/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34327.9727 - mae: 135.6223 - val_loss: 8942.8955 - val_mae: 82.1902\n",
      "Epoch 95/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35728.7461 - mae: 120.1758 - val_loss: 2895.9644 - val_mae: 45.2685\n",
      "Epoch 96/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33474.4492 - mae: 131.1555 - val_loss: 26135.9160 - val_mae: 143.2443\n",
      "Epoch 97/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35337.9570 - mae: 131.8357 - val_loss: 1183.2041 - val_mae: 26.5916\n",
      "Epoch 98/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37367.8398 - mae: 139.4477 - val_loss: 5396.8022 - val_mae: 62.9604\n",
      "Epoch 99/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35024.8008 - mae: 135.5578 - val_loss: 1504.4392 - val_mae: 30.8399\n",
      "Epoch 100/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34101.8281 - mae: 132.4752 - val_loss: 860.9061 - val_mae: 23.6310\n",
      "Epoch 101/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32962.9961 - mae: 125.8740 - val_loss: 4472.0293 - val_mae: 57.0249\n",
      "Epoch 102/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35183.3125 - mae: 124.5252 - val_loss: 1114.2405 - val_mae: 26.5715\n",
      "Epoch 103/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36579.7383 - mae: 136.2605 - val_loss: 1712.0187 - val_mae: 32.1774\n",
      "Epoch 104/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31402.5488 - mae: 124.1770 - val_loss: 77532.5547 - val_mae: 244.9901\n",
      "Epoch 105/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34871.8555 - mae: 120.9305 - val_loss: 7534.0864 - val_mae: 73.0771\n",
      "Epoch 106/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38052.5000 - mae: 121.4479 - val_loss: 1019.2892 - val_mae: 23.0173\n",
      "Epoch 107/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31986.8496 - mae: 111.2544 - val_loss: 1267.7948 - val_mae: 27.7887\n",
      "Epoch 108/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 37236.3984 - mae: 129.1116 - val_loss: 33247.3711 - val_mae: 154.6486\n",
      "Epoch 109/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32328.0781 - mae: 130.2803 - val_loss: 15990.3066 - val_mae: 111.7181\n",
      "Epoch 110/2000\n",
      "88/88 [==============================] - ETA: 0s - loss: 38541.5781 - mae: 142.828 - 0s 2ms/step - loss: 32454.5000 - mae: 124.3780 - val_loss: 228744.2656 - val_mae: 418.0118\n",
      "Epoch 111/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35056.5977 - mae: 132.7957 - val_loss: 15454.8008 - val_mae: 106.8907\n",
      "Epoch 112/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38992.2422 - mae: 131.9649 - val_loss: 1612.0785 - val_mae: 31.4278\n",
      "Epoch 113/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30606.4648 - mae: 113.2792 - val_loss: 1656.6835 - val_mae: 32.2175\n",
      "Epoch 114/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36723.9688 - mae: 126.6279 - val_loss: 2468.8862 - val_mae: 39.9553\n",
      "Epoch 115/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31032.5977 - mae: 120.5378 - val_loss: 1023.1262 - val_mae: 25.1852\n",
      "Epoch 116/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 37609.0625 - mae: 105.6753 - val_loss: 49120.5000 - val_mae: 194.0928\n",
      "Epoch 117/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29680.2012 - mae: 103.7021 - val_loss: 8031.7949 - val_mae: 79.0079\n",
      "Epoch 118/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 37031.1016 - mae: 148.3657 - val_loss: 173299.2344 - val_mae: 363.8388\n",
      "Epoch 119/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32210.0664 - mae: 134.3899 - val_loss: 12460.2363 - val_mae: 97.2222\n",
      "Epoch 120/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33198.4141 - mae: 118.5081 - val_loss: 973.2458 - val_mae: 23.2174\n",
      "Epoch 121/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38830.7148 - mae: 128.8217 - val_loss: 6604.7280 - val_mae: 68.1172\n",
      "Epoch 122/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32541.2793 - mae: 122.7141 - val_loss: 181071.5469 - val_mae: 374.6514\n",
      "Epoch 123/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33254.3125 - mae: 137.7522 - val_loss: 1898.6459 - val_mae: 35.5298\n",
      "Epoch 124/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33527.0547 - mae: 134.6335 - val_loss: 5595.8516 - val_mae: 65.9736\n",
      "Epoch 125/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34576.3672 - mae: 138.4259 - val_loss: 84646.2891 - val_mae: 254.3978\n",
      "Epoch 126/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33707.8242 - mae: 135.2475 - val_loss: 3873.5378 - val_mae: 53.4462\n",
      "Epoch 127/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36901.2305 - mae: 131.5841 - val_loss: 4155.4961 - val_mae: 54.8354\n",
      "Epoch 128/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31975.7402 - mae: 131.1504 - val_loss: 25582.9355 - val_mae: 142.0594\n",
      "Epoch 129/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35118.1953 - mae: 123.8356 - val_loss: 96631.7266 - val_mae: 275.6214\n",
      "Epoch 130/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35778.6406 - mae: 140.3950 - val_loss: 2182.2839 - val_mae: 37.6079\n",
      "Epoch 131/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31261.2773 - mae: 129.3108 - val_loss: 154860.9531 - val_mae: 346.2292\n",
      "Epoch 132/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35133.8516 - mae: 115.5862 - val_loss: 29622.2852 - val_mae: 150.9471\n",
      "Epoch 133/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35259.6367 - mae: 127.0263 - val_loss: 22726.2305 - val_mae: 134.4051\n",
      "Epoch 134/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35739.3086 - mae: 127.7835 - val_loss: 80788.9219 - val_mae: 252.6329\n",
      "Epoch 135/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33068.6055 - mae: 103.9552 - val_loss: 554.3549 - val_mae: 18.9617\n",
      "Epoch 136/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32455.0234 - mae: 115.6102 - val_loss: 3195.3086 - val_mae: 47.2331\n",
      "Epoch 137/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35668.9102 - mae: 138.0336 - val_loss: 55051.0820 - val_mae: 208.1325\n",
      "Epoch 138/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33708.6016 - mae: 136.4731 - val_loss: 14617.3828 - val_mae: 105.0646\n",
      "Epoch 139/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33969.8516 - mae: 118.0854 - val_loss: 112495.2500 - val_mae: 297.9032\n",
      "Epoch 140/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35214.3008 - mae: 139.1665 - val_loss: 241569.2812 - val_mae: 439.5243\n",
      "Epoch 141/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31659.8730 - mae: 106.1531 - val_loss: 32833.8516 - val_mae: 158.3841\n",
      "Epoch 142/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35078.2891 - mae: 119.3716 - val_loss: 469.0468 - val_mae: 17.5777\n",
      "Epoch 143/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30639.3496 - mae: 118.1925 - val_loss: 114254.1797 - val_mae: 297.7557\n",
      "Epoch 144/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34868.9922 - mae: 112.7273 - val_loss: 9662.1006 - val_mae: 87.6010\n",
      "Epoch 145/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34638.9219 - mae: 132.2440 - val_loss: 97102.5234 - val_mae: 276.2751\n",
      "Epoch 146/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35560.3633 - mae: 131.3069 - val_loss: 4284.2012 - val_mae: 56.9560\n",
      "Epoch 147/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34341.4883 - mae: 128.4676 - val_loss: 7407.3984 - val_mae: 73.5385\n",
      "Epoch 148/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32652.7910 - mae: 114.6315 - val_loss: 42043.1836 - val_mae: 179.2555\n",
      "Epoch 149/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36178.4648 - mae: 122.5618 - val_loss: 78581.5625 - val_mae: 244.2348\n",
      "Epoch 150/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32412.2344 - mae: 105.2685 - val_loss: 30679.7520 - val_mae: 155.9932\n",
      "Epoch 151/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34180.2422 - mae: 130.1098 - val_loss: 481.4500 - val_mae: 16.5653\n",
      "Epoch 152/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32895.2656 - mae: 117.0880 - val_loss: 9781.0352 - val_mae: 86.8927\n",
      "Epoch 153/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36148.9141 - mae: 116.8055 - val_loss: 7427.6123 - val_mae: 75.3847\n",
      "Epoch 154/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31805.0645 - mae: 126.5646 - val_loss: 11408.7041 - val_mae: 92.6685\n",
      "Epoch 155/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33044.0586 - mae: 101.4748 - val_loss: 626.9974 - val_mae: 20.4695\n",
      "Epoch 156/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34599.0586 - mae: 121.1987 - val_loss: 890.6500 - val_mae: 23.8080\n",
      "Epoch 157/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37781.4141 - mae: 133.1916 - val_loss: 1065.4313 - val_mae: 25.9662\n",
      "Epoch 158/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32778.8477 - mae: 97.9359 - val_loss: 6031.3208 - val_mae: 67.8346\n",
      "Epoch 159/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36188.6836 - mae: 129.8800 - val_loss: 920.6573 - val_mae: 24.0008\n",
      "Epoch 160/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32669.6211 - mae: 135.2538 - val_loss: 20825.6562 - val_mae: 125.8476\n",
      "Epoch 161/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31558.7324 - mae: 121.4061 - val_loss: 219132.9219 - val_mae: 410.1528\n",
      "Epoch 162/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35178.3555 - mae: 120.0309 - val_loss: 23108.5430 - val_mae: 133.6430\n",
      "Epoch 163/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30910.1094 - mae: 124.6231 - val_loss: 336623.3750 - val_mae: 510.8397\n",
      "Epoch 164/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36833.1602 - mae: 129.7739 - val_loss: 489.8033 - val_mae: 16.5646\n",
      "Epoch 165/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33468.9766 - mae: 112.9442 - val_loss: 59495.2695 - val_mae: 214.8613\n",
      "Epoch 166/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36518.4570 - mae: 117.5706 - val_loss: 2237.9429 - val_mae: 39.2640\n",
      "Epoch 167/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30918.7793 - mae: 93.6302 - val_loss: 1991.3812 - val_mae: 37.4320\n",
      "Epoch 168/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36502.5430 - mae: 134.2894 - val_loss: 3014.3428 - val_mae: 48.3130\n",
      "Epoch 169/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30329.5410 - mae: 95.5604 - val_loss: 101186.5547 - val_mae: 281.6157\n",
      "Epoch 170/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 2ms/step - loss: 33959.7070 - mae: 121.8885 - val_loss: 378.2859 - val_mae: 15.0011\n",
      "Epoch 171/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35986.4531 - mae: 124.9076 - val_loss: 5540.7129 - val_mae: 65.5700\n",
      "Epoch 172/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36705.7266 - mae: 142.0708 - val_loss: 12848.4961 - val_mae: 99.3089\n",
      "Epoch 173/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31334.8770 - mae: 116.6671 - val_loss: 66366.1875 - val_mae: 223.9482\n",
      "Epoch 174/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33149.5742 - mae: 107.3535 - val_loss: 345.8812 - val_mae: 12.8554\n",
      "Epoch 175/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35098.5156 - mae: 104.0593 - val_loss: 1224.6183 - val_mae: 29.2876\n",
      "Epoch 176/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31323.5938 - mae: 110.6975 - val_loss: 2348.9141 - val_mae: 42.2118\n",
      "Epoch 177/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35162.9023 - mae: 136.4593 - val_loss: 21492.8125 - val_mae: 127.6229\n",
      "Epoch 178/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34039.8125 - mae: 129.7588 - val_loss: 18816.4824 - val_mae: 119.3014\n",
      "Epoch 179/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34428.5273 - mae: 120.2307 - val_loss: 9431.9609 - val_mae: 87.7936\n",
      "Epoch 180/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32482.0293 - mae: 128.0518 - val_loss: 3786.6973 - val_mae: 52.1666\n",
      "Epoch 181/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34912.5000 - mae: 127.2798 - val_loss: 2780.7444 - val_mae: 45.0963\n",
      "Epoch 182/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32809.6211 - mae: 117.2080 - val_loss: 5701.8872 - val_mae: 66.2651\n",
      "Epoch 183/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36360.6680 - mae: 136.1223 - val_loss: 3274.0676 - val_mae: 49.8919\n",
      "Epoch 184/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32612.6953 - mae: 124.2254 - val_loss: 277.7426 - val_mae: 12.4196\n",
      "Epoch 185/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32275.3301 - mae: 103.8819 - val_loss: 205073.7656 - val_mae: 398.1683\n",
      "Epoch 186/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 37002.1914 - mae: 142.5243 - val_loss: 1559.5206 - val_mae: 32.6900\n",
      "Epoch 187/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34510.2422 - mae: 134.9575 - val_loss: 90273.3594 - val_mae: 263.6649\n",
      "Epoch 188/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32159.1230 - mae: 113.1617 - val_loss: 768.9525 - val_mae: 22.8151\n",
      "Epoch 189/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31957.5371 - mae: 123.0151 - val_loss: 56715.5430 - val_mae: 210.1449\n",
      "Epoch 190/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 37081.4609 - mae: 142.6077 - val_loss: 251.7915 - val_mae: 12.6864\n",
      "Epoch 191/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33696.1719 - mae: 118.8582 - val_loss: 3094.7771 - val_mae: 46.9682\n",
      "Epoch 192/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33794.2188 - mae: 146.6353 - val_loss: 3689.2893 - val_mae: 52.4978\n",
      "Epoch 193/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33756.9609 - mae: 129.3798 - val_loss: 4492.0557 - val_mae: 57.4921\n",
      "Epoch 194/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34288.8320 - mae: 120.3336 - val_loss: 34295.5703 - val_mae: 163.2757\n",
      "Epoch 195/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35345.6445 - mae: 141.1284 - val_loss: 698.3317 - val_mae: 22.3716\n",
      "Epoch 196/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33432.1445 - mae: 123.1180 - val_loss: 51828.5703 - val_mae: 203.3806\n",
      "Epoch 197/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31387.7246 - mae: 111.7812 - val_loss: 17876.3633 - val_mae: 119.1817\n",
      "Epoch 198/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37899.8320 - mae: 146.3880 - val_loss: 4784.8267 - val_mae: 57.3029\n",
      "Epoch 199/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32292.6328 - mae: 133.2773 - val_loss: 6784.4126 - val_mae: 72.4886\n",
      "Epoch 200/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34896.2109 - mae: 140.2099 - val_loss: 15810.7803 - val_mae: 112.7577\n",
      "Epoch 201/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33278.4297 - mae: 124.3405 - val_loss: 1132.7998 - val_mae: 28.2013\n",
      "Epoch 202/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34104.8672 - mae: 148.1968 - val_loss: 50037.8320 - val_mae: 196.5064\n",
      "Epoch 203/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33564.1680 - mae: 116.4097 - val_loss: 465.7957 - val_mae: 17.1991\n",
      "Epoch 204/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34265.4961 - mae: 107.4458 - val_loss: 211.0752 - val_mae: 11.9616\n",
      "Epoch 205/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34427.5742 - mae: 104.4782 - val_loss: 80120.7031 - val_mae: 248.6679\n",
      "Epoch 206/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33465.7539 - mae: 134.6919 - val_loss: 6351.1060 - val_mae: 70.5569\n",
      "Epoch 207/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36158.7344 - mae: 130.6538 - val_loss: 249.4238 - val_mae: 11.4506\n",
      "Epoch 208/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32627.3418 - mae: 121.4022 - val_loss: 50000.9844 - val_mae: 198.8479\n",
      "Epoch 209/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31456.2461 - mae: 119.6259 - val_loss: 391.9180 - val_mae: 16.5408\n",
      "Epoch 210/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35094.9180 - mae: 122.1059 - val_loss: 38660.2500 - val_mae: 172.9447\n",
      "Epoch 211/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33050.2266 - mae: 123.2720 - val_loss: 4338.1284 - val_mae: 56.9263\n",
      "Epoch 212/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36529.9258 - mae: 129.5961 - val_loss: 569.2239 - val_mae: 19.9558\n",
      "Epoch 213/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31476.8125 - mae: 106.8046 - val_loss: 27068.5059 - val_mae: 144.5206\n",
      "Epoch 214/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35282.9102 - mae: 141.7651 - val_loss: 252.4744 - val_mae: 12.6224\n",
      "Epoch 215/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32383.4609 - mae: 111.8276 - val_loss: 188.0851 - val_mae: 10.5176\n",
      "Epoch 216/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34705.4531 - mae: 122.1473 - val_loss: 20771.0352 - val_mae: 125.5229\n",
      "Epoch 217/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35592.8711 - mae: 125.8488 - val_loss: 225.5328 - val_mae: 12.1108\n",
      "Epoch 218/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32427.6074 - mae: 101.8807 - val_loss: 4314.5181 - val_mae: 57.5902\n",
      "Epoch 219/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34265.4414 - mae: 97.5709 - val_loss: 359.6284 - val_mae: 15.9096\n",
      "Epoch 220/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35166.2305 - mae: 138.6246 - val_loss: 399.7615 - val_mae: 15.2727\n",
      "Epoch 221/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32601.6953 - mae: 126.5191 - val_loss: 249216.0625 - val_mae: 441.2421\n",
      "Epoch 222/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34392.3711 - mae: 119.9219 - val_loss: 644.1805 - val_mae: 21.6923\n",
      "Epoch 223/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33017.9023 - mae: 113.6927 - val_loss: 49750.8281 - val_mae: 192.7943\n",
      "Epoch 224/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33853.7617 - mae: 128.6371 - val_loss: 8883.8984 - val_mae: 81.8313\n",
      "Epoch 225/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35359.6602 - mae: 144.6760 - val_loss: 281.7893 - val_mae: 11.8414\n",
      "Epoch 226/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 29817.4297 - mae: 124.1429 - val_loss: 68159.5938 - val_mae: 231.8454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36879.2109 - mae: 128.9930 - val_loss: 188.9035 - val_mae: 11.5128\n",
      "Epoch 228/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30485.1602 - mae: 83.0997 - val_loss: 33026.6133 - val_mae: 160.5113\n",
      "Epoch 229/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33893.8711 - mae: 112.2977 - val_loss: 736.6339 - val_mae: 22.1909\n",
      "Epoch 230/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35685.6445 - mae: 117.6268 - val_loss: 5408.0771 - val_mae: 67.5102\n",
      "Epoch 231/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33630.8164 - mae: 100.6308 - val_loss: 197.5607 - val_mae: 11.6606\n",
      "Epoch 232/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31010.9512 - mae: 98.5769 - val_loss: 249936.5469 - val_mae: 437.0990\n",
      "Epoch 233/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34606.7383 - mae: 136.5598 - val_loss: 12617.7139 - val_mae: 98.5144\n",
      "Epoch 234/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34499.6680 - mae: 118.0844 - val_loss: 216.4275 - val_mae: 11.8481\n",
      "Epoch 235/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31503.0371 - mae: 107.8766 - val_loss: 242643.5625 - val_mae: 434.6097\n",
      "Epoch 236/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34666.5156 - mae: 133.7112 - val_loss: 55901.8672 - val_mae: 207.8015\n",
      "Epoch 237/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37377.5820 - mae: 98.5122 - val_loss: 12064.2607 - val_mae: 97.3417\n",
      "Epoch 238/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32242.2930 - mae: 131.5866 - val_loss: 76195.5469 - val_mae: 246.5951\n",
      "Epoch 239/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32366.3418 - mae: 137.8877 - val_loss: 57649.3320 - val_mae: 214.2868\n",
      "Epoch 240/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31950.7598 - mae: 114.8823 - val_loss: 401.2699 - val_mae: 16.9946\n",
      "Epoch 241/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32733.0254 - mae: 104.8119 - val_loss: 52445.8945 - val_mae: 203.7548\n",
      "Epoch 242/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35252.8750 - mae: 106.9888 - val_loss: 3219.0085 - val_mae: 48.9180\n",
      "Epoch 243/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33447.6406 - mae: 120.3540 - val_loss: 194.6297 - val_mae: 11.0101\n",
      "Epoch 244/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36015.5312 - mae: 134.3959 - val_loss: 165.9886 - val_mae: 8.8058\n",
      "Epoch 245/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33476.6445 - mae: 119.0348 - val_loss: 15998.2891 - val_mae: 112.4196\n",
      "Epoch 246/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30881.5488 - mae: 130.7404 - val_loss: 58939.4219 - val_mae: 213.6319\n",
      "Epoch 247/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34096.1445 - mae: 145.2202 - val_loss: 12395.2178 - val_mae: 99.3308\n",
      "Epoch 248/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36315.2031 - mae: 132.0620 - val_loss: 224.9736 - val_mae: 12.4891\n",
      "Epoch 249/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36264.3477 - mae: 142.4470 - val_loss: 683.9109 - val_mae: 23.2975\n",
      "Epoch 250/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32100.4238 - mae: 131.1716 - val_loss: 8916.6855 - val_mae: 83.5182\n",
      "Epoch 251/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31673.2520 - mae: 101.9159 - val_loss: 312889.2188 - val_mae: 493.7858\n",
      "Epoch 252/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33784.1992 - mae: 127.5087 - val_loss: 125149.6797 - val_mae: 310.2881\n",
      "Epoch 253/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34214.0625 - mae: 128.7306 - val_loss: 666.4631 - val_mae: 22.1230\n",
      "Epoch 254/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35031.9844 - mae: 131.0931 - val_loss: 41237.2461 - val_mae: 176.8961\n",
      "Epoch 255/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32320.2402 - mae: 91.4424 - val_loss: 24575.5762 - val_mae: 136.6635\n",
      "Epoch 256/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33631.1055 - mae: 116.0005 - val_loss: 6278.5015 - val_mae: 71.6844\n",
      "Epoch 257/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33030.0117 - mae: 121.5666 - val_loss: 205001.7812 - val_mae: 398.1751\n",
      "Epoch 258/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32150.0469 - mae: 113.6342 - val_loss: 33817.0352 - val_mae: 162.9167\n",
      "Epoch 259/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36041.6797 - mae: 136.5262 - val_loss: 1456.6454 - val_mae: 33.5364\n",
      "Epoch 260/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34023.4414 - mae: 135.4744 - val_loss: 1347.0837 - val_mae: 33.3947\n",
      "Epoch 261/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32044.7441 - mae: 128.5750 - val_loss: 31759.8477 - val_mae: 157.2922\n",
      "Epoch 262/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33442.6211 - mae: 116.7023 - val_loss: 1382.5347 - val_mae: 33.0940\n",
      "Epoch 263/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33587.9492 - mae: 132.5974 - val_loss: 20239.5859 - val_mae: 123.9336\n",
      "Epoch 264/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32887.8477 - mae: 130.4581 - val_loss: 15854.1455 - val_mae: 109.2654\n",
      "Epoch 265/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36235.0391 - mae: 107.3191 - val_loss: 88.4367 - val_mae: 7.3052\n",
      "Epoch 266/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33319.7070 - mae: 100.5118 - val_loss: 92187.9609 - val_mae: 270.5315\n",
      "Epoch 267/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29584.7246 - mae: 109.5193 - val_loss: 17961.6309 - val_mae: 117.3342\n",
      "Epoch 268/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36649.0156 - mae: 124.5498 - val_loss: 17339.3730 - val_mae: 114.9150\n",
      "Epoch 269/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31413.4629 - mae: 128.2791 - val_loss: 38667.3867 - val_mae: 174.0500\n",
      "Epoch 270/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34008.2070 - mae: 131.9709 - val_loss: 116297.5859 - val_mae: 301.7565\n",
      "Epoch 271/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34155.1211 - mae: 128.1241 - val_loss: 209316.5312 - val_mae: 404.0582\n",
      "Epoch 272/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34788.0898 - mae: 116.8337 - val_loss: 2875.9800 - val_mae: 47.1369\n",
      "Epoch 273/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32034.7207 - mae: 106.0146 - val_loss: 175.7659 - val_mae: 10.8983\n",
      "Epoch 274/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36774.6328 - mae: 127.0852 - val_loss: 1655.9819 - val_mae: 36.6421\n",
      "Epoch 275/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32683.5449 - mae: 118.8210 - val_loss: 98.0154 - val_mae: 7.8930\n",
      "Epoch 276/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36124.0781 - mae: 126.5038 - val_loss: 14969.4658 - val_mae: 107.6226\n",
      "Epoch 277/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 26563.1465 - mae: 91.4907 - val_loss: 88398.2266 - val_mae: 262.3288\n",
      "Epoch 278/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35790.5977 - mae: 122.6601 - val_loss: 43510.9258 - val_mae: 185.6609\n",
      "Epoch 279/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34302.2930 - mae: 136.0706 - val_loss: 145.3225 - val_mae: 8.3638\n",
      "Epoch 280/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33358.5547 - mae: 120.9533 - val_loss: 4179.0820 - val_mae: 55.5333\n",
      "Epoch 281/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37364.5312 - mae: 139.3905 - val_loss: 267520.1562 - val_mae: 450.2637\n",
      "Epoch 282/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31582.1738 - mae: 105.6505 - val_loss: 29053.3652 - val_mae: 149.0990\n",
      "Epoch 283/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31437.7578 - mae: 93.6423 - val_loss: 634.6227 - val_mae: 21.5565\n",
      "Epoch 284/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33031.4805 - mae: 124.9040 - val_loss: 2066.6997 - val_mae: 38.0456\n",
      "Epoch 285/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33081.6680 - mae: 125.0710 - val_loss: 18434.3438 - val_mae: 120.0820\n",
      "Epoch 286/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34375.2344 - mae: 140.2106 - val_loss: 13337.7354 - val_mae: 99.3294\n",
      "Epoch 287/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35130.8281 - mae: 124.7199 - val_loss: 528.9484 - val_mae: 20.5467\n",
      "Epoch 288/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34199.6055 - mae: 113.1505 - val_loss: 86.6853 - val_mae: 6.7390\n",
      "Epoch 289/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33153.0312 - mae: 111.0570 - val_loss: 39939.6172 - val_mae: 174.4076\n",
      "Epoch 290/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34759.9102 - mae: 132.5538 - val_loss: 1821.4191 - val_mae: 38.5363\n",
      "Epoch 291/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31049.4141 - mae: 91.8619 - val_loss: 23798.6602 - val_mae: 134.4270\n",
      "Epoch 292/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34658.5898 - mae: 134.3328 - val_loss: 135579.6406 - val_mae: 321.2557\n",
      "Epoch 293/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32119.7402 - mae: 106.7622 - val_loss: 49799.0508 - val_mae: 200.4758\n",
      "Epoch 294/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32279.2148 - mae: 115.8253 - val_loss: 20914.5527 - val_mae: 124.9551\n",
      "Epoch 295/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34371.7461 - mae: 114.3009 - val_loss: 4689.2749 - val_mae: 58.8775\n",
      "Epoch 296/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34663.1992 - mae: 134.3192 - val_loss: 918.3961 - val_mae: 26.0836\n",
      "Epoch 297/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37535.4727 - mae: 107.0392 - val_loss: 888.2498 - val_mae: 25.6123\n",
      "Epoch 298/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29630.3477 - mae: 87.2131 - val_loss: 4231.5977 - val_mae: 58.4759\n",
      "Epoch 299/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32631.9590 - mae: 120.9090 - val_loss: 18504.2676 - val_mae: 119.2158\n",
      "Epoch 300/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33415.7188 - mae: 109.3326 - val_loss: 31772.1953 - val_mae: 155.7562\n",
      "Epoch 301/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36314.6914 - mae: 135.6326 - val_loss: 55.3360 - val_mae: 5.6481\n",
      "Epoch 302/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 30888.8164 - mae: 109.1821 - val_loss: 12235.7148 - val_mae: 98.0634\n",
      "Epoch 303/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36069.2188 - mae: 131.9703 - val_loss: 91.7238 - val_mae: 7.8166\n",
      "Epoch 304/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28430.0820 - mae: 85.9870 - val_loss: 41026.8750 - val_mae: 178.7794\n",
      "Epoch 305/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34295.5898 - mae: 106.8420 - val_loss: 350.1489 - val_mae: 14.3233\n",
      "Epoch 306/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35380.5898 - mae: 112.1629 - val_loss: 1959.4355 - val_mae: 36.4360\n",
      "Epoch 307/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30670.5059 - mae: 125.8392 - val_loss: 243350.1250 - val_mae: 433.9903\n",
      "Epoch 308/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35624.4922 - mae: 114.1044 - val_loss: 69.1438 - val_mae: 6.2493\n",
      "Epoch 309/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34204.0156 - mae: 124.2349 - val_loss: 214.9225 - val_mae: 12.7225\n",
      "Epoch 310/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34306.5703 - mae: 122.0889 - val_loss: 144.5474 - val_mae: 10.0272\n",
      "Epoch 311/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31487.7539 - mae: 121.4042 - val_loss: 91.3852 - val_mae: 7.4810\n",
      "Epoch 312/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31670.3301 - mae: 100.1855 - val_loss: 735.0090 - val_mae: 23.4323\n",
      "Epoch 313/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34468.0273 - mae: 117.7503 - val_loss: 3876.8376 - val_mae: 52.7815\n",
      "Epoch 314/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34288.5703 - mae: 127.6525 - val_loss: 63347.5625 - val_mae: 223.8989\n",
      "Epoch 315/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32039.7246 - mae: 115.0218 - val_loss: 52450.5469 - val_mae: 202.7948\n",
      "Epoch 316/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34612.6172 - mae: 117.3915 - val_loss: 2221.6997 - val_mae: 41.7491\n",
      "Epoch 317/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31060.5156 - mae: 109.8019 - val_loss: 276908.1875 - val_mae: 464.1778\n",
      "Epoch 318/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33819.0000 - mae: 115.5255 - val_loss: 9005.1182 - val_mae: 85.2974\n",
      "Epoch 319/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33765.9766 - mae: 122.3895 - val_loss: 101143.3906 - val_mae: 279.3750\n",
      "Epoch 320/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31677.2793 - mae: 124.1846 - val_loss: 88615.7656 - val_mae: 261.3677\n",
      "Epoch 321/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35775.7656 - mae: 123.5087 - val_loss: 59125.0625 - val_mae: 212.4152\n",
      "Epoch 322/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 31496.4219 - mae: 106.2826 - val_loss: 6926.2173 - val_mae: 71.5854\n",
      "Epoch 323/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36610.5508 - mae: 142.5143 - val_loss: 490.1516 - val_mae: 17.9107\n",
      "Epoch 324/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 30799.3164 - mae: 135.5194 - val_loss: 25008.0723 - val_mae: 139.1025\n",
      "Epoch 325/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33182.7734 - mae: 113.6227 - val_loss: 98288.1953 - val_mae: 277.7927\n",
      "Epoch 326/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33141.6914 - mae: 124.3389 - val_loss: 1415.4674 - val_mae: 33.5165\n",
      "Epoch 327/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35428.7930 - mae: 127.1912 - val_loss: 1996.2783 - val_mae: 38.8797\n",
      "Epoch 328/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30242.9082 - mae: 122.2991 - val_loss: 175340.3125 - val_mae: 372.5692\n",
      "Epoch 329/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35841.2500 - mae: 116.4773 - val_loss: 126460.0156 - val_mae: 316.6958\n",
      "Epoch 330/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33800.1172 - mae: 109.6976 - val_loss: 69798.4062 - val_mae: 231.5882\n",
      "Epoch 331/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32219.1074 - mae: 119.9423 - val_loss: 700.8245 - val_mae: 23.7227\n",
      "Epoch 332/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33644.5430 - mae: 123.9964 - val_loss: 44089.5781 - val_mae: 183.6609\n",
      "Epoch 333/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30948.6074 - mae: 120.0760 - val_loss: 37620.5352 - val_mae: 168.7184\n",
      "Epoch 334/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35854.0859 - mae: 105.7710 - val_loss: 564.9525 - val_mae: 21.6452\n",
      "Epoch 335/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33766.5039 - mae: 131.8571 - val_loss: 124659.1406 - val_mae: 312.1599\n",
      "Epoch 336/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34062.1797 - mae: 126.4221 - val_loss: 2404.5591 - val_mae: 41.1135\n",
      "Epoch 337/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31920.7695 - mae: 126.5016 - val_loss: 81416.9844 - val_mae: 252.4397\n",
      "Epoch 338/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35578.8672 - mae: 141.1946 - val_loss: 126.3164 - val_mae: 9.3901\n",
      "Epoch 339/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33890.2305 - mae: 114.4316 - val_loss: 1072.6971 - val_mae: 28.3813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30998.2246 - mae: 96.7008 - val_loss: 2293.0955 - val_mae: 43.3059\n",
      "Epoch 341/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32387.1992 - mae: 96.7003 - val_loss: 5657.4585 - val_mae: 66.1838\n",
      "Epoch 342/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37049.3242 - mae: 139.2093 - val_loss: 67425.8125 - val_mae: 228.6803\n",
      "Epoch 343/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33683.4258 - mae: 115.7798 - val_loss: 314.6563 - val_mae: 15.7881\n",
      "Epoch 344/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35562.4453 - mae: 106.9950 - val_loss: 22498.6543 - val_mae: 134.1812\n",
      "Epoch 345/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32179.4121 - mae: 131.2094 - val_loss: 9947.6904 - val_mae: 90.4575\n",
      "Epoch 346/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30040.2773 - mae: 94.3732 - val_loss: 202.3022 - val_mae: 12.4844\n",
      "Epoch 347/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34782.6172 - mae: 135.7289 - val_loss: 689.1162 - val_mae: 23.6844\n",
      "Epoch 348/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33607.9961 - mae: 107.6666 - val_loss: 201504.2344 - val_mae: 397.8199\n",
      "Epoch 349/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34856.5977 - mae: 106.7137 - val_loss: 60.8152 - val_mae: 5.9138\n",
      "Epoch 350/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33567.3086 - mae: 135.7134 - val_loss: 76709.6953 - val_mae: 243.3990\n",
      "Epoch 351/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34365.1992 - mae: 119.4073 - val_loss: 23064.3184 - val_mae: 135.4768\n",
      "Epoch 352/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33443.8438 - mae: 119.4958 - val_loss: 153.0583 - val_mae: 10.4322\n",
      "Epoch 353/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31381.8477 - mae: 104.1686 - val_loss: 39837.1211 - val_mae: 176.1580\n",
      "Epoch 354/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33541.8242 - mae: 119.3729 - val_loss: 486.3967 - val_mae: 18.8336\n",
      "Epoch 355/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34608.9023 - mae: 97.2002 - val_loss: 67.6782 - val_mae: 6.7621\n",
      "Epoch 356/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36220.4883 - mae: 141.0816 - val_loss: 621.3287 - val_mae: 20.7009\n",
      "Epoch 357/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30818.3613 - mae: 91.6916 - val_loss: 5463.7021 - val_mae: 67.3312\n",
      "Epoch 358/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32261.5977 - mae: 115.8318 - val_loss: 27.9098 - val_mae: 3.8756\n",
      "Epoch 359/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34437.3906 - mae: 139.7721 - val_loss: 13615.9258 - val_mae: 103.9234\n",
      "Epoch 360/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33391.6211 - mae: 138.3389 - val_loss: 8772.4238 - val_mae: 84.4076\n",
      "Epoch 361/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33965.6758 - mae: 140.4661 - val_loss: 2126.8364 - val_mae: 42.6464\n",
      "Epoch 362/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31961.0098 - mae: 117.3434 - val_loss: 43372.1328 - val_mae: 181.7647\n",
      "Epoch 363/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33994.8164 - mae: 106.3314 - val_loss: 2163.2488 - val_mae: 42.2000\n",
      "Epoch 364/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32341.9219 - mae: 113.4765 - val_loss: 59120.8516 - val_mae: 216.0285\n",
      "Epoch 365/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37630.7031 - mae: 129.4952 - val_loss: 19649.9336 - val_mae: 125.6303\n",
      "Epoch 366/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31007.5254 - mae: 124.6913 - val_loss: 11050.7285 - val_mae: 94.9942\n",
      "Epoch 367/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33954.2188 - mae: 118.5331 - val_loss: 414.7033 - val_mae: 18.7367\n",
      "Epoch 368/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33659.9102 - mae: 106.9967 - val_loss: 2938.0837 - val_mae: 47.6567\n",
      "Epoch 369/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32988.5508 - mae: 120.9114 - val_loss: 21837.9160 - val_mae: 131.1631\n",
      "Epoch 370/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36953.2539 - mae: 135.4400 - val_loss: 127.7656 - val_mae: 9.2308\n",
      "Epoch 371/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31581.6152 - mae: 119.6954 - val_loss: 26369.2480 - val_mae: 140.3218\n",
      "Epoch 372/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35194.7070 - mae: 118.7016 - val_loss: 58445.0898 - val_mae: 210.8372\n",
      "Epoch 373/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34962.1562 - mae: 132.4294 - val_loss: 137023.3281 - val_mae: 322.9578\n",
      "Epoch 374/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29325.0312 - mae: 103.4738 - val_loss: 20873.6875 - val_mae: 125.7965\n",
      "Epoch 375/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34489.6406 - mae: 142.1915 - val_loss: 220448.9062 - val_mae: 414.5764\n",
      "Epoch 376/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37122.3125 - mae: 137.8891 - val_loss: 8549.4834 - val_mae: 83.8504\n",
      "Epoch 377/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34265.2812 - mae: 141.7744 - val_loss: 25.4707 - val_mae: 4.2099\n",
      "Epoch 378/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33795.9883 - mae: 99.8414 - val_loss: 295.0966 - val_mae: 15.1665\n",
      "Epoch 379/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31059.1465 - mae: 105.0086 - val_loss: 31622.3008 - val_mae: 155.6809\n",
      "Epoch 380/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35091.8281 - mae: 137.9863 - val_loss: 102572.0391 - val_mae: 280.6115\n",
      "Epoch 381/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31094.6309 - mae: 120.6690 - val_loss: 73247.1250 - val_mae: 238.5258\n",
      "Epoch 382/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35271.5234 - mae: 140.5341 - val_loss: 106.9793 - val_mae: 8.4472\n",
      "Epoch 383/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30977.2109 - mae: 81.2176 - val_loss: 95.6634 - val_mae: 8.0051\n",
      "Epoch 384/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32527.0098 - mae: 115.3447 - val_loss: 190.1034 - val_mae: 11.2516\n",
      "Epoch 385/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33638.2422 - mae: 106.7201 - val_loss: 48255.8516 - val_mae: 192.8783\n",
      "Epoch 386/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32603.8477 - mae: 125.9268 - val_loss: 191.5021 - val_mae: 9.3855\n",
      "Epoch 387/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35883.1523 - mae: 126.5408 - val_loss: 73.9787 - val_mae: 6.8740\n",
      "Epoch 388/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33253.3086 - mae: 101.9411 - val_loss: 130.0893 - val_mae: 9.5642\n",
      "Epoch 389/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30033.3945 - mae: 111.7029 - val_loss: 1492.5468 - val_mae: 36.5853\n",
      "Epoch 390/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37800.0469 - mae: 141.0553 - val_loss: 2913.9468 - val_mae: 45.9263\n",
      "Epoch 391/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31430.1016 - mae: 103.9008 - val_loss: 65.9008 - val_mae: 5.6990\n",
      "Epoch 392/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32678.2422 - mae: 110.4893 - val_loss: 3499.9519 - val_mae: 51.9338\n",
      "Epoch 393/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35914.4102 - mae: 141.3443 - val_loss: 6561.0913 - val_mae: 72.2906\n",
      "Epoch 394/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31461.2402 - mae: 128.0761 - val_loss: 14530.2197 - val_mae: 105.2901\n",
      "Epoch 395/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32676.8535 - mae: 133.6544 - val_loss: 137322.1406 - val_mae: 328.9845\n",
      "Epoch 396/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33780.6914 - mae: 104.8590 - val_loss: 18452.3574 - val_mae: 119.5340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33645.1523 - mae: 108.3995 - val_loss: 153.2095 - val_mae: 10.6437\n",
      "Epoch 398/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31296.0566 - mae: 96.0928 - val_loss: 1490.2045 - val_mae: 33.5919\n",
      "Epoch 399/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35598.7617 - mae: 125.5717 - val_loss: 106386.2031 - val_mae: 284.0642\n",
      "Epoch 400/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32447.4531 - mae: 123.0837 - val_loss: 192.4348 - val_mae: 11.9399\n",
      "Epoch 401/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31640.4980 - mae: 91.0589 - val_loss: 229391.6719 - val_mae: 422.9708\n",
      "Epoch 402/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29924.8555 - mae: 106.4515 - val_loss: 10950.7881 - val_mae: 94.5892\n",
      "Epoch 403/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36028.0508 - mae: 122.2729 - val_loss: 131518.8750 - val_mae: 318.6899\n",
      "Epoch 404/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30919.2617 - mae: 120.9258 - val_loss: 7469.7212 - val_mae: 74.4649\n",
      "Epoch 405/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34650.6953 - mae: 135.4695 - val_loss: 28095.9590 - val_mae: 150.4911\n",
      "Epoch 406/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33922.7617 - mae: 143.1957 - val_loss: 187379.5625 - val_mae: 382.0779\n",
      "Epoch 407/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30708.4297 - mae: 86.3979 - val_loss: 35789.6641 - val_mae: 167.4709\n",
      "Epoch 408/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32630.9473 - mae: 119.1351 - val_loss: 6725.4126 - val_mae: 71.3117\n",
      "Epoch 409/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33937.3359 - mae: 117.6475 - val_loss: 8076.0029 - val_mae: 81.2099\n",
      "Epoch 410/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33588.1250 - mae: 111.2556 - val_loss: 27557.3750 - val_mae: 147.9306\n",
      "Epoch 411/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33266.1914 - mae: 115.7902 - val_loss: 132.4202 - val_mae: 9.3154\n",
      "Epoch 412/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30673.0098 - mae: 97.8264 - val_loss: 42204.4062 - val_mae: 183.2821\n",
      "Epoch 413/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33958.5273 - mae: 113.5987 - val_loss: 303.6895 - val_mae: 14.6860\n",
      "Epoch 414/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33005.0742 - mae: 129.5470 - val_loss: 261415.0156 - val_mae: 449.2590\n",
      "Epoch 415/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31459.7676 - mae: 102.7070 - val_loss: 12593.0586 - val_mae: 99.7546\n",
      "Epoch 416/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35487.2070 - mae: 127.1202 - val_loss: 1456.5708 - val_mae: 34.7081\n",
      "Epoch 417/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35700.0898 - mae: 111.8652 - val_loss: 41.2735 - val_mae: 4.9481\n",
      "Epoch 418/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30715.5469 - mae: 95.2904 - val_loss: 114083.5859 - val_mae: 298.7929\n",
      "Epoch 419/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32676.7812 - mae: 132.9712 - val_loss: 90859.4688 - val_mae: 266.5424\n",
      "Epoch 420/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32826.5312 - mae: 132.5342 - val_loss: 28754.5625 - val_mae: 150.2635\n",
      "Epoch 421/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35522.4336 - mae: 106.8223 - val_loss: 463.5863 - val_mae: 19.4642\n",
      "Epoch 422/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33720.9727 - mae: 107.9118 - val_loss: 20989.8086 - val_mae: 127.1453\n",
      "Epoch 423/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31893.8066 - mae: 91.0050 - val_loss: 53294.3594 - val_mae: 201.3345\n",
      "Epoch 424/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29283.7344 - mae: 104.5055 - val_loss: 7767.2959 - val_mae: 78.1618\n",
      "Epoch 425/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37090.9492 - mae: 100.6074 - val_loss: 206.1573 - val_mae: 12.8339\n",
      "Epoch 426/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31796.2969 - mae: 126.2846 - val_loss: 27478.8672 - val_mae: 142.7342\n",
      "Epoch 427/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33591.3945 - mae: 115.8670 - val_loss: 1092.4579 - val_mae: 28.8728\n",
      "Epoch 428/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31619.3418 - mae: 101.6778 - val_loss: 296.5477 - val_mae: 15.0845\n",
      "Epoch 429/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35279.5352 - mae: 104.2019 - val_loss: 278.7160 - val_mae: 13.3123\n",
      "Epoch 430/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33349.6797 - mae: 123.9223 - val_loss: 69296.6484 - val_mae: 227.0500\n",
      "Epoch 431/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30708.0996 - mae: 129.1696 - val_loss: 372158.7188 - val_mae: 535.9129\n",
      "Epoch 432/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36719.0195 - mae: 108.4183 - val_loss: 67.2672 - val_mae: 6.5152\n",
      "Epoch 433/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34641.4648 - mae: 121.7809 - val_loss: 57766.2734 - val_mae: 215.6133\n",
      "Epoch 434/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29385.8633 - mae: 114.7134 - val_loss: 271209.4375 - val_mae: 460.6476\n",
      "Epoch 435/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35438.1406 - mae: 105.8375 - val_loss: 57.3767 - val_mae: 5.7743\n",
      "Epoch 436/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30410.6367 - mae: 94.5351 - val_loss: 101981.8906 - val_mae: 282.4717\n",
      "Epoch 437/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33639.6406 - mae: 128.1327 - val_loss: 714.7888 - val_mae: 23.3216\n",
      "Epoch 438/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32182.4219 - mae: 99.6156 - val_loss: 26050.3789 - val_mae: 141.0955\n",
      "Epoch 439/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34658.6953 - mae: 118.4166 - val_loss: 12.3569 - val_mae: 2.7393\n",
      "Epoch 440/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31993.7578 - mae: 123.9096 - val_loss: 11871.0186 - val_mae: 94.8837\n",
      "Epoch 441/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35407.3906 - mae: 133.1080 - val_loss: 33080.7930 - val_mae: 160.4045\n",
      "Epoch 442/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32569.9102 - mae: 131.0723 - val_loss: 10429.7803 - val_mae: 89.7215\n",
      "Epoch 443/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33981.6016 - mae: 101.8368 - val_loss: 9796.3633 - val_mae: 85.5513\n",
      "Epoch 444/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32012.1582 - mae: 129.7499 - val_loss: 299443.9375 - val_mae: 481.7344\n",
      "Epoch 445/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32001.0449 - mae: 86.8855 - val_loss: 62854.3711 - val_mae: 221.5464\n",
      "Epoch 446/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35197.6445 - mae: 137.5073 - val_loss: 19703.3750 - val_mae: 123.2577\n",
      "Epoch 447/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36328.2852 - mae: 142.9312 - val_loss: 54105.1484 - val_mae: 203.5576\n",
      "Epoch 448/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33848.0391 - mae: 127.2514 - val_loss: 537.1426 - val_mae: 19.5816\n",
      "Epoch 449/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35453.8164 - mae: 124.1304 - val_loss: 165.5067 - val_mae: 10.7208\n",
      "Epoch 450/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32115.2070 - mae: 115.1847 - val_loss: 72103.7578 - val_mae: 236.2186\n",
      "Epoch 451/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32429.9492 - mae: 129.8084 - val_loss: 11450.5957 - val_mae: 96.2546\n",
      "Epoch 452/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36612.6484 - mae: 126.7384 - val_loss: 16.7518 - val_mae: 3.2859\n",
      "Epoch 453/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30556.9082 - mae: 126.5596 - val_loss: 742.4812 - val_mae: 23.5799\n",
      "Epoch 454/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32014.0039 - mae: 136.2485 - val_loss: 115568.2812 - val_mae: 299.6984\n",
      "Epoch 455/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35218.6172 - mae: 116.8056 - val_loss: 277.9296 - val_mae: 13.2096\n",
      "Epoch 456/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33228.7461 - mae: 96.4743 - val_loss: 5447.3989 - val_mae: 64.4047\n",
      "Epoch 457/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35534.3672 - mae: 129.3833 - val_loss: 17.0682 - val_mae: 3.0355\n",
      "Epoch 458/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32424.1523 - mae: 120.8659 - val_loss: 1147.1650 - val_mae: 28.7174\n",
      "Epoch 459/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35450.4414 - mae: 118.1401 - val_loss: 7143.6167 - val_mae: 73.5895\n",
      "Epoch 460/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33139.5430 - mae: 131.2040 - val_loss: 2153.8281 - val_mae: 40.4432\n",
      "Epoch 461/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32316.9629 - mae: 102.1727 - val_loss: 129340.1406 - val_mae: 313.9919\n",
      "Epoch 462/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32337.0527 - mae: 118.6620 - val_loss: 14902.8115 - val_mae: 106.8965\n",
      "Epoch 463/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34451.8398 - mae: 143.7896 - val_loss: 3805.2666 - val_mae: 52.6325\n",
      "Epoch 464/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34323.8672 - mae: 117.8916 - val_loss: 11678.3887 - val_mae: 96.9984\n",
      "Epoch 465/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35434.1406 - mae: 118.5060 - val_loss: 18.5956 - val_mae: 2.9907\n",
      "Epoch 466/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31550.6797 - mae: 135.0244 - val_loss: 78420.3438 - val_mae: 250.7949\n",
      "Epoch 467/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32647.1133 - mae: 114.2455 - val_loss: 8042.4800 - val_mae: 81.1648\n",
      "Epoch 468/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32826.6875 - mae: 99.0113 - val_loss: 44.7411 - val_mae: 5.2991\n",
      "Epoch 469/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33340.4766 - mae: 131.6017 - val_loss: 29845.8789 - val_mae: 151.4750\n",
      "Epoch 470/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35521.9688 - mae: 124.1274 - val_loss: 4150.3022 - val_mae: 54.5462\n",
      "Epoch 471/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32017.7715 - mae: 116.5529 - val_loss: 14660.6484 - val_mae: 105.5924\n",
      "Epoch 472/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34060.1055 - mae: 122.8554 - val_loss: 7989.2192 - val_mae: 79.6768\n",
      "Epoch 473/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32541.3691 - mae: 120.5467 - val_loss: 238.0443 - val_mae: 14.7154\n",
      "Epoch 474/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33949.2500 - mae: 118.7691 - val_loss: 89442.6016 - val_mae: 262.3990\n",
      "Epoch 475/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35567.5898 - mae: 109.4532 - val_loss: 54726.6094 - val_mae: 206.6152\n",
      "Epoch 476/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 27825.0371 - mae: 110.2672 - val_loss: 1365.6168 - val_mae: 32.9980\n",
      "Epoch 477/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36967.7734 - mae: 140.4813 - val_loss: 58872.1680 - val_mae: 215.3898\n",
      "Epoch 478/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32899.1953 - mae: 134.7098 - val_loss: 77702.9062 - val_mae: 246.5398\n",
      "Epoch 479/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32945.0234 - mae: 134.8871 - val_loss: 271751.7188 - val_mae: 455.6079\n",
      "Epoch 480/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34484.9258 - mae: 111.5981 - val_loss: 9621.3916 - val_mae: 87.2408\n",
      "Epoch 481/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35321.7656 - mae: 133.3430 - val_loss: 2116.3411 - val_mae: 40.3791\n",
      "Epoch 482/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28998.0449 - mae: 100.5141 - val_loss: 87366.4609 - val_mae: 261.5694\n",
      "Epoch 483/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35223.1484 - mae: 125.3551 - val_loss: 7712.7769 - val_mae: 77.0849\n",
      "Epoch 484/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33613.6914 - mae: 136.9917 - val_loss: 116351.7656 - val_mae: 298.4493\n",
      "Epoch 485/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32666.9414 - mae: 109.2303 - val_loss: 28654.2910 - val_mae: 149.3166\n",
      "Epoch 486/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36405.8203 - mae: 130.8493 - val_loss: 43.8056 - val_mae: 5.3260\n",
      "Epoch 487/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31051.6172 - mae: 85.5550 - val_loss: 14955.6738 - val_mae: 106.9057\n",
      "Epoch 488/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 37273.3398 - mae: 110.0131 - val_loss: 46.8548 - val_mae: 5.6083\n",
      "Epoch 489/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31824.1680 - mae: 117.0544 - val_loss: 99110.2344 - val_mae: 278.7094\n",
      "Epoch 490/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31839.8086 - mae: 107.7618 - val_loss: 2106.9463 - val_mae: 39.9682\n",
      "Epoch 491/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35057.3086 - mae: 122.7465 - val_loss: 114652.3906 - val_mae: 296.5783\n",
      "Epoch 492/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32787.7812 - mae: 116.5162 - val_loss: 7309.3462 - val_mae: 76.4787\n",
      "Epoch 493/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34823.1797 - mae: 144.8170 - val_loss: 13878.2070 - val_mae: 104.0093\n",
      "Epoch 494/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31734.4824 - mae: 100.7153 - val_loss: 443.2186 - val_mae: 18.2725\n",
      "Epoch 495/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35810.9336 - mae: 146.1134 - val_loss: 2360.8298 - val_mae: 43.6485\n",
      "Epoch 496/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31327.2031 - mae: 121.5263 - val_loss: 53693.5859 - val_mae: 205.0584\n",
      "Epoch 497/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32084.1484 - mae: 118.5096 - val_loss: 202245.7344 - val_mae: 395.3055\n",
      "Epoch 498/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32919.5508 - mae: 81.8399 - val_loss: 277448.6562 - val_mae: 463.6490\n",
      "Epoch 499/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35111.7773 - mae: 98.6232 - val_loss: 41.2132 - val_mae: 4.8806\n",
      "Epoch 500/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31681.1562 - mae: 128.5537 - val_loss: 24341.2812 - val_mae: 137.1658\n",
      "Epoch 501/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35193.2695 - mae: 141.9534 - val_loss: 249118.7344 - val_mae: 435.9200\n",
      "Epoch 502/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31394.7695 - mae: 110.7803 - val_loss: 5797.0552 - val_mae: 64.3798\n",
      "Epoch 503/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34378.5508 - mae: 112.3844 - val_loss: 85370.9688 - val_mae: 257.8031\n",
      "Epoch 504/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36061.6992 - mae: 137.9893 - val_loss: 26.8444 - val_mae: 4.2362\n",
      "Epoch 505/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31091.5957 - mae: 107.0234 - val_loss: 136533.0156 - val_mae: 326.6485\n",
      "Epoch 506/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34968.9414 - mae: 117.3793 - val_loss: 5207.0884 - val_mae: 63.4006\n",
      "Epoch 507/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31944.2324 - mae: 139.2775 - val_loss: 49754.0430 - val_mae: 195.6579\n",
      "Epoch 508/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35026.5586 - mae: 123.9506 - val_loss: 434.7292 - val_mae: 17.6960\n",
      "Epoch 509/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35012.7148 - mae: 146.2954 - val_loss: 123905.9062 - val_mae: 307.8534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 510/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30997.3965 - mae: 114.8253 - val_loss: 41644.3281 - val_mae: 180.7341\n",
      "Epoch 511/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36132.4727 - mae: 132.8037 - val_loss: 977.8302 - val_mae: 25.6262\n",
      "Epoch 512/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30110.0840 - mae: 112.5477 - val_loss: 57558.2539 - val_mae: 210.0626\n",
      "Epoch 513/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38700.5703 - mae: 133.2520 - val_loss: 963.7401 - val_mae: 25.6151\n",
      "Epoch 514/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29472.5605 - mae: 114.3344 - val_loss: 20984.9590 - val_mae: 128.5379\n",
      "Epoch 515/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36219.8945 - mae: 125.9515 - val_loss: 71.1998 - val_mae: 6.8776\n",
      "Epoch 516/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32954.8867 - mae: 123.4748 - val_loss: 4532.4590 - val_mae: 57.7521\n",
      "Epoch 517/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33092.3477 - mae: 126.9753 - val_loss: 22.0771 - val_mae: 3.8468\n",
      "Epoch 518/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32181.4727 - mae: 85.2954 - val_loss: 74.9916 - val_mae: 7.6467\n",
      "Epoch 519/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32564.9688 - mae: 117.6986 - val_loss: 84811.1484 - val_mae: 255.8391\n",
      "Epoch 520/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35106.0664 - mae: 108.3517 - val_loss: 26161.9590 - val_mae: 144.9588\n",
      "Epoch 521/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34956.0820 - mae: 130.3769 - val_loss: 3448.0098 - val_mae: 52.0991\n",
      "Epoch 522/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32122.2715 - mae: 97.5976 - val_loss: 27116.6562 - val_mae: 146.9182\n",
      "Epoch 523/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33247.9102 - mae: 134.0269 - val_loss: 23972.3105 - val_mae: 138.1884\n",
      "Epoch 524/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32770.3203 - mae: 111.7508 - val_loss: 3595.6277 - val_mae: 51.2592\n",
      "Epoch 525/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34254.4883 - mae: 114.2047 - val_loss: 14361.7666 - val_mae: 105.2994\n",
      "Epoch 526/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34290.1250 - mae: 144.8997 - val_loss: 243.6029 - val_mae: 12.2504\n",
      "Epoch 527/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36740.1562 - mae: 132.5575 - val_loss: 3499.4707 - val_mae: 55.2422\n",
      "Epoch 528/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30221.7461 - mae: 103.3194 - val_loss: 4909.3057 - val_mae: 60.4899\n",
      "Epoch 529/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33583.1367 - mae: 109.4234 - val_loss: 450.7709 - val_mae: 17.5294\n",
      "Epoch 530/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37038.2852 - mae: 137.5645 - val_loss: 523.8522 - val_mae: 18.3053\n",
      "Epoch 531/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 27581.0820 - mae: 92.3087 - val_loss: 87275.5000 - val_mae: 260.4575\n",
      "Epoch 532/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34223.1055 - mae: 126.9035 - val_loss: 170865.9531 - val_mae: 366.4476\n",
      "Epoch 533/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34763.4219 - mae: 110.5265 - val_loss: 194337.1562 - val_mae: 386.3656\n",
      "Epoch 534/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34740.5312 - mae: 121.5959 - val_loss: 1371.6553 - val_mae: 32.1637\n",
      "Epoch 535/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30528.4160 - mae: 112.5446 - val_loss: 112434.3906 - val_mae: 295.8279\n",
      "Epoch 536/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32264.7559 - mae: 109.9490 - val_loss: 20435.0137 - val_mae: 129.8956\n",
      "Epoch 537/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34930.1055 - mae: 136.4855 - val_loss: 3922.0801 - val_mae: 55.8407\n",
      "Epoch 538/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34097.2383 - mae: 124.6972 - val_loss: 428.8595 - val_mae: 17.7889\n",
      "Epoch 539/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33348.6836 - mae: 113.7129 - val_loss: 774.2346 - val_mae: 24.7793\n",
      "Epoch 540/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32543.9688 - mae: 130.1946 - val_loss: 67.4748 - val_mae: 6.6109\n",
      "Epoch 541/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37056.5625 - mae: 146.5351 - val_loss: 10632.8750 - val_mae: 91.8257\n",
      "Epoch 542/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32523.5391 - mae: 88.0262 - val_loss: 11.3113 - val_mae: 2.7173\n",
      "Epoch 543/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31657.5801 - mae: 83.4079 - val_loss: 39820.3750 - val_mae: 176.2184\n",
      "Epoch 544/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34939.0977 - mae: 124.0711 - val_loss: 5152.1172 - val_mae: 60.9202\n",
      "Epoch 545/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35383.4453 - mae: 131.5672 - val_loss: 138.5942 - val_mae: 9.1053\n",
      "Epoch 546/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 27291.6934 - mae: 71.3060 - val_loss: 178264.6406 - val_mae: 373.3739\n",
      "Epoch 547/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34985.9453 - mae: 128.4316 - val_loss: 1250.4474 - val_mae: 30.0113\n",
      "Epoch 548/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36804.9258 - mae: 136.5568 - val_loss: 29.6978 - val_mae: 4.3915\n",
      "Epoch 549/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30327.9824 - mae: 100.5371 - val_loss: 857.0291 - val_mae: 24.2485\n",
      "Epoch 550/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31852.5840 - mae: 114.5522 - val_loss: 12152.9219 - val_mae: 94.2218\n",
      "Epoch 551/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34533.9531 - mae: 119.9441 - val_loss: 30065.5723 - val_mae: 151.1093\n",
      "Epoch 552/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30443.1035 - mae: 118.0017 - val_loss: 19646.2305 - val_mae: 125.6521\n",
      "Epoch 553/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36530.7656 - mae: 132.8018 - val_loss: 13860.6064 - val_mae: 104.2255\n",
      "Epoch 554/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33422.8711 - mae: 133.1878 - val_loss: 107.1232 - val_mae: 8.6115\n",
      "Epoch 555/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31701.2461 - mae: 117.8096 - val_loss: 32113.8008 - val_mae: 160.9422\n",
      "Epoch 556/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35289.8281 - mae: 140.4746 - val_loss: 20120.9395 - val_mae: 124.6805\n",
      "Epoch 557/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33558.9297 - mae: 144.4742 - val_loss: 76796.5391 - val_mae: 244.8267\n",
      "Epoch 558/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35633.6992 - mae: 129.3833 - val_loss: 62916.2031 - val_mae: 224.1729\n",
      "Epoch 559/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33219.1484 - mae: 124.3761 - val_loss: 14963.9951 - val_mae: 106.2589\n",
      "Epoch 560/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31628.3008 - mae: 103.5269 - val_loss: 1210.6376 - val_mae: 27.7501\n",
      "Epoch 561/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33765.6992 - mae: 127.8634 - val_loss: 90.6275 - val_mae: 7.3968\n",
      "Epoch 562/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36657.7930 - mae: 143.9866 - val_loss: 46.8687 - val_mae: 4.7916\n",
      "Epoch 563/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31807.2812 - mae: 105.0186 - val_loss: 14006.2236 - val_mae: 104.7994\n",
      "Epoch 564/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33728.2188 - mae: 116.7716 - val_loss: 14426.9648 - val_mae: 107.3047\n",
      "Epoch 565/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32775.6172 - mae: 111.2030 - val_loss: 4844.9141 - val_mae: 62.7046\n",
      "Epoch 566/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34218.8047 - mae: 134.3067 - val_loss: 9869.9473 - val_mae: 91.9912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 567/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34563.5938 - mae: 98.4936 - val_loss: 1017.7060 - val_mae: 26.9248\n",
      "Epoch 568/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33238.6445 - mae: 116.8661 - val_loss: 92.0986 - val_mae: 7.2954\n",
      "Epoch 569/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30530.6094 - mae: 98.4762 - val_loss: 76011.1562 - val_mae: 241.6414\n",
      "Epoch 570/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33262.5898 - mae: 121.1939 - val_loss: 121978.3203 - val_mae: 306.5895\n",
      "Epoch 571/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34342.3555 - mae: 127.5769 - val_loss: 46.4594 - val_mae: 5.5905\n",
      "Epoch 572/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33190.8594 - mae: 127.8145 - val_loss: 33696.6797 - val_mae: 164.3809\n",
      "Epoch 573/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34598.4023 - mae: 131.0606 - val_loss: 4451.2593 - val_mae: 57.3647\n",
      "Epoch 574/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34792.8086 - mae: 119.6349 - val_loss: 7.7528 - val_mae: 2.2049\n",
      "Epoch 575/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34796.2812 - mae: 130.7488 - val_loss: 43.5338 - val_mae: 4.9810\n",
      "Epoch 576/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32156.4355 - mae: 97.2958 - val_loss: 999.6213 - val_mae: 28.5054\n",
      "Epoch 577/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35237.1289 - mae: 100.5699 - val_loss: 5947.1890 - val_mae: 68.2452\n",
      "Epoch 578/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31554.5938 - mae: 112.0909 - val_loss: 283.2099 - val_mae: 12.9786\n",
      "Epoch 579/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30386.7363 - mae: 96.0831 - val_loss: 6635.4419 - val_mae: 74.0449\n",
      "Epoch 580/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34620.2578 - mae: 134.6287 - val_loss: 3724.6516 - val_mae: 51.5588\n",
      "Epoch 581/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34947.3477 - mae: 130.4973 - val_loss: 1764.6313 - val_mae: 35.8343\n",
      "Epoch 582/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31882.2930 - mae: 132.2805 - val_loss: 500.6059 - val_mae: 20.0421\n",
      "Epoch 583/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33776.1445 - mae: 117.4568 - val_loss: 6219.9326 - val_mae: 70.9004\n",
      "Epoch 584/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34362.2266 - mae: 137.5259 - val_loss: 18578.7246 - val_mae: 119.3125\n",
      "Epoch 585/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34729.2656 - mae: 118.5835 - val_loss: 76437.9688 - val_mae: 245.4341\n",
      "Epoch 586/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31535.9980 - mae: 121.3473 - val_loss: 26029.8926 - val_mae: 141.1624\n",
      "Epoch 587/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32715.0664 - mae: 111.5599 - val_loss: 3368.6536 - val_mae: 51.4242\n",
      "Epoch 588/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33798.5469 - mae: 113.1404 - val_loss: 115400.0625 - val_mae: 298.1309\n",
      "Epoch 589/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32040.6133 - mae: 114.5948 - val_loss: 115227.4922 - val_mae: 299.8648\n",
      "Epoch 590/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32962.1133 - mae: 121.5774 - val_loss: 43.1653 - val_mae: 5.2547\n",
      "Epoch 591/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34205.4531 - mae: 116.3222 - val_loss: 3397.0928 - val_mae: 50.0187\n",
      "Epoch 592/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33945.8789 - mae: 131.0631 - val_loss: 219516.0312 - val_mae: 409.6261\n",
      "Epoch 593/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34328.3867 - mae: 119.5504 - val_loss: 2008.7075 - val_mae: 41.3344\n",
      "Epoch 594/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32558.1895 - mae: 94.9257 - val_loss: 15448.2373 - val_mae: 112.6418\n",
      "Epoch 595/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34527.8672 - mae: 128.1117 - val_loss: 610.3685 - val_mae: 21.8656\n",
      "Epoch 596/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34024.8633 - mae: 104.1797 - val_loss: 72.1484 - val_mae: 6.2806\n",
      "Epoch 597/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34145.3789 - mae: 108.4155 - val_loss: 6657.5874 - val_mae: 72.3018\n",
      "Epoch 598/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29530.2383 - mae: 96.3349 - val_loss: 1374.9573 - val_mae: 31.3239\n",
      "Epoch 599/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34247.2852 - mae: 122.8602 - val_loss: 29.7233 - val_mae: 4.4470\n",
      "Epoch 600/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30953.9844 - mae: 104.0740 - val_loss: 205927.7969 - val_mae: 400.6955\n",
      "Epoch 601/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33318.5469 - mae: 113.1699 - val_loss: 156010.8594 - val_mae: 348.7852\n",
      "Epoch 602/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31345.2246 - mae: 119.3335 - val_loss: 3301.9922 - val_mae: 46.6924\n",
      "Epoch 603/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32685.3691 - mae: 120.0618 - val_loss: 3566.5139 - val_mae: 50.3715\n",
      "Epoch 604/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33924.7109 - mae: 129.8197 - val_loss: 34951.6289 - val_mae: 164.6128\n",
      "Epoch 605/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34840.0156 - mae: 130.5555 - val_loss: 5291.5850 - val_mae: 64.9853\n",
      "Epoch 606/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37853.4219 - mae: 111.0063 - val_loss: 2935.0503 - val_mae: 46.2427\n",
      "Epoch 607/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29232.0137 - mae: 97.6620 - val_loss: 841.3904 - val_mae: 25.1589\n",
      "Epoch 608/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34579.1992 - mae: 131.9195 - val_loss: 5996.5156 - val_mae: 68.3315\n",
      "Epoch 609/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34157.8477 - mae: 119.7972 - val_loss: 80.0685 - val_mae: 7.7137\n",
      "Epoch 610/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31611.2246 - mae: 110.3283 - val_loss: 43784.9102 - val_mae: 185.5994\n",
      "Epoch 611/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32074.8066 - mae: 120.7474 - val_loss: 146709.9531 - val_mae: 337.6274\n",
      "Epoch 612/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35043.4023 - mae: 148.1275 - val_loss: 15491.7393 - val_mae: 113.6289\n",
      "Epoch 613/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34623.2969 - mae: 128.0760 - val_loss: 8061.7422 - val_mae: 81.1262\n",
      "Epoch 614/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35943.2148 - mae: 143.5329 - val_loss: 60852.2500 - val_mae: 221.1952\n",
      "Epoch 615/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31430.2754 - mae: 110.9784 - val_loss: 15612.4707 - val_mae: 112.1066\n",
      "Epoch 616/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35464.6797 - mae: 145.0878 - val_loss: 56.8854 - val_mae: 5.8059\n",
      "Epoch 617/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34074.5352 - mae: 94.2924 - val_loss: 4870.9082 - val_mae: 63.3184\n",
      "Epoch 618/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32503.8633 - mae: 105.9132 - val_loss: 5579.7480 - val_mae: 67.5277\n",
      "Epoch 619/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32573.9062 - mae: 111.2654 - val_loss: 9852.3516 - val_mae: 86.2194\n",
      "Epoch 620/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35984.3594 - mae: 117.8443 - val_loss: 8970.6318 - val_mae: 83.9048\n",
      "Epoch 621/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 28494.8828 - mae: 105.4506 - val_loss: 187681.6250 - val_mae: 379.4811\n",
      "Epoch 622/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35450.0312 - mae: 135.4900 - val_loss: 861.6189 - val_mae: 25.1736\n",
      "Epoch 623/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32056.2148 - mae: 118.8949 - val_loss: 9534.4609 - val_mae: 83.8573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 624/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33478.7070 - mae: 132.6668 - val_loss: 1911.1743 - val_mae: 35.5245\n",
      "Epoch 625/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33107.6914 - mae: 103.5767 - val_loss: 11862.8730 - val_mae: 96.7513\n",
      "Epoch 626/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 29655.5469 - mae: 97.7890 - val_loss: 1804.0178 - val_mae: 39.6813\n",
      "Epoch 627/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36550.2656 - mae: 101.0552 - val_loss: 705.9545 - val_mae: 23.6188\n",
      "Epoch 628/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33001.0234 - mae: 128.3592 - val_loss: 36741.6016 - val_mae: 169.3012\n",
      "Epoch 629/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34505.6641 - mae: 122.2021 - val_loss: 8136.2852 - val_mae: 81.4789\n",
      "Epoch 630/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30970.1797 - mae: 97.8062 - val_loss: 76088.5781 - val_mae: 244.1940\n",
      "Epoch 631/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35048.3867 - mae: 118.2180 - val_loss: 6259.5332 - val_mae: 70.3456\n",
      "Epoch 632/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32481.7949 - mae: 125.7749 - val_loss: 4443.9204 - val_mae: 57.0262\n",
      "Epoch 633/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34519.0820 - mae: 130.0483 - val_loss: 23213.0645 - val_mae: 133.3220\n",
      "Epoch 634/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33079.4805 - mae: 108.6219 - val_loss: 274651.1250 - val_mae: 463.7391\n",
      "Epoch 635/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32437.9473 - mae: 115.2718 - val_loss: 9.8317 - val_mae: 2.4496\n",
      "Epoch 636/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33019.0156 - mae: 114.7830 - val_loss: 33148.8711 - val_mae: 160.0105\n",
      "Epoch 637/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34008.2148 - mae: 127.4786 - val_loss: 12230.7178 - val_mae: 99.6665\n",
      "Epoch 638/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34944.3594 - mae: 136.4673 - val_loss: 3382.7654 - val_mae: 51.3067\n",
      "Epoch 639/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29156.2773 - mae: 111.7566 - val_loss: 98720.1562 - val_mae: 277.0614\n",
      "Epoch 640/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36394.1328 - mae: 107.2917 - val_loss: 9.7844 - val_mae: 2.5086\n",
      "Epoch 641/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31846.4395 - mae: 124.5040 - val_loss: 86680.8828 - val_mae: 261.6068\n",
      "Epoch 642/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32139.3379 - mae: 113.2385 - val_loss: 11251.5391 - val_mae: 95.9338\n",
      "Epoch 643/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32330.0449 - mae: 126.7082 - val_loss: 16.7573 - val_mae: 3.2844\n",
      "Epoch 644/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31839.5742 - mae: 123.1525 - val_loss: 6038.5215 - val_mae: 69.0956\n",
      "Epoch 645/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34319.1562 - mae: 115.1059 - val_loss: 65639.6562 - val_mae: 224.5225\n",
      "Epoch 646/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35001.8242 - mae: 125.3515 - val_loss: 1247.1287 - val_mae: 31.9026\n",
      "Epoch 647/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33348.3008 - mae: 140.8898 - val_loss: 64256.2383 - val_mae: 226.9079\n",
      "Epoch 648/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35221.7734 - mae: 117.1288 - val_loss: 5338.0581 - val_mae: 59.9391\n",
      "Epoch 649/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30669.4629 - mae: 92.0628 - val_loss: 1659.1125 - val_mae: 36.6723\n",
      "Epoch 650/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35047.5117 - mae: 128.6328 - val_loss: 24550.2070 - val_mae: 139.6703\n",
      "Epoch 651/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36504.7852 - mae: 136.0263 - val_loss: 7875.7056 - val_mae: 76.1156\n",
      "Epoch 652/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29069.1875 - mae: 94.2129 - val_loss: 259183.7812 - val_mae: 445.9140\n",
      "Epoch 653/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31225.8379 - mae: 82.5044 - val_loss: 238690.5000 - val_mae: 429.5395\n",
      "Epoch 654/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30709.0918 - mae: 112.0667 - val_loss: 47511.3047 - val_mae: 191.9548\n",
      "Epoch 655/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35065.7109 - mae: 120.2128 - val_loss: 42.8791 - val_mae: 5.3335\n",
      "Epoch 656/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35738.8789 - mae: 136.2464 - val_loss: 1406.1917 - val_mae: 32.1412\n",
      "Epoch 657/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32783.9531 - mae: 121.3284 - val_loss: 30265.4902 - val_mae: 156.4001\n",
      "Epoch 658/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31259.1152 - mae: 113.5275 - val_loss: 552.6987 - val_mae: 20.9201\n",
      "Epoch 659/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36081.0781 - mae: 107.0983 - val_loss: 390.3188 - val_mae: 16.4031\n",
      "Epoch 660/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34549.7656 - mae: 125.9591 - val_loss: 975.1243 - val_mae: 27.4098\n",
      "Epoch 661/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29276.5723 - mae: 102.0153 - val_loss: 25132.8164 - val_mae: 141.7125\n",
      "Epoch 662/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34338.6250 - mae: 116.5883 - val_loss: 85958.2500 - val_mae: 257.4255\n",
      "Epoch 663/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34833.5586 - mae: 126.5414 - val_loss: 9096.9385 - val_mae: 81.8398\n",
      "Epoch 664/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36115.1055 - mae: 124.4633 - val_loss: 42827.4570 - val_mae: 179.0880\n",
      "Epoch 665/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29449.7383 - mae: 120.3958 - val_loss: 101697.9844 - val_mae: 279.3367\n",
      "Epoch 666/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35732.5430 - mae: 118.8750 - val_loss: 58805.6719 - val_mae: 214.3633\n",
      "Epoch 667/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32793.4414 - mae: 123.5573 - val_loss: 623.5002 - val_mae: 22.5890\n",
      "Epoch 668/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34753.2461 - mae: 145.2496 - val_loss: 84483.1641 - val_mae: 257.8992\n",
      "Epoch 669/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30742.5879 - mae: 114.8158 - val_loss: 19065.6035 - val_mae: 123.3110\n",
      "Epoch 670/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35944.3359 - mae: 106.7541 - val_loss: 1316.8605 - val_mae: 32.8932\n",
      "Epoch 671/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30974.2070 - mae: 128.8039 - val_loss: 14753.4678 - val_mae: 108.8168\n",
      "Epoch 672/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36480.9648 - mae: 110.0738 - val_loss: 1150.0649 - val_mae: 30.4094\n",
      "Epoch 673/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31143.0000 - mae: 95.7861 - val_loss: 1567.9586 - val_mae: 34.4656\n",
      "Epoch 674/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30277.5996 - mae: 111.6349 - val_loss: 14024.1973 - val_mae: 104.8546\n",
      "Epoch 675/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36663.5273 - mae: 129.9677 - val_loss: 13635.0703 - val_mae: 100.1408\n",
      "Epoch 676/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31505.3750 - mae: 131.9932 - val_loss: 274702.9062 - val_mae: 462.4048\n",
      "Epoch 677/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31399.6211 - mae: 103.7581 - val_loss: 3047.3838 - val_mae: 50.7045\n",
      "Epoch 678/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 39442.2344 - mae: 117.7925 - val_loss: 18.2727 - val_mae: 3.4445\n",
      "Epoch 679/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 26632.0176 - mae: 90.1041 - val_loss: 22085.2207 - val_mae: 131.1402\n",
      "Epoch 680/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36713.2930 - mae: 126.8951 - val_loss: 684.9557 - val_mae: 21.7812\n",
      "Epoch 681/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35393.9766 - mae: 125.8749 - val_loss: 534.3478 - val_mae: 19.2398\n",
      "Epoch 682/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31788.5527 - mae: 123.1853 - val_loss: 18592.1738 - val_mae: 120.0140\n",
      "Epoch 683/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33102.1797 - mae: 127.0468 - val_loss: 81.9677 - val_mae: 7.8714\n",
      "Epoch 684/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34937.8594 - mae: 145.1532 - val_loss: 13922.3564 - val_mae: 104.8621\n",
      "Epoch 685/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34091.8750 - mae: 110.3463 - val_loss: 2260.8552 - val_mae: 38.7809\n",
      "Epoch 686/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30948.9160 - mae: 113.7431 - val_loss: 254589.3281 - val_mae: 444.4388\n",
      "Epoch 687/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33323.8789 - mae: 113.1194 - val_loss: 64178.5430 - val_mae: 223.6344\n",
      "Epoch 688/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30842.6094 - mae: 113.2515 - val_loss: 49820.5781 - val_mae: 196.5206\n",
      "Epoch 689/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36062.6914 - mae: 143.5784 - val_loss: 2730.9282 - val_mae: 45.8813\n",
      "Epoch 690/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35327.6406 - mae: 108.6737 - val_loss: 101.1225 - val_mae: 7.7094\n",
      "Epoch 691/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32387.7852 - mae: 102.0219 - val_loss: 55530.8867 - val_mae: 209.5048\n",
      "Epoch 692/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32827.7031 - mae: 118.4716 - val_loss: 38453.7930 - val_mae: 174.6346\n",
      "Epoch 693/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33156.5820 - mae: 141.3649 - val_loss: 46973.0117 - val_mae: 191.4449\n",
      "Epoch 694/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34067.2305 - mae: 135.6986 - val_loss: 14252.0420 - val_mae: 105.1701\n",
      "Epoch 695/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34520.4336 - mae: 148.0888 - val_loss: 12076.4150 - val_mae: 98.7591\n",
      "Epoch 696/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34475.1914 - mae: 139.7704 - val_loss: 1552.3950 - val_mae: 34.7092\n",
      "Epoch 697/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32805.8555 - mae: 126.8359 - val_loss: 30042.5078 - val_mae: 152.3224\n",
      "Epoch 698/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34591.2305 - mae: 132.7929 - val_loss: 80720.7344 - val_mae: 248.0509\n",
      "Epoch 699/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33623.0391 - mae: 138.8314 - val_loss: 235.1227 - val_mae: 14.0399\n",
      "Epoch 700/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36853.3359 - mae: 115.3452 - val_loss: 102711.1328 - val_mae: 282.9039\n",
      "Epoch 701/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29936.4746 - mae: 109.5698 - val_loss: 67076.8047 - val_mae: 229.4483\n",
      "Epoch 702/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34550.9961 - mae: 113.5616 - val_loss: 18.6420 - val_mae: 3.4427\n",
      "Epoch 703/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34989.8242 - mae: 122.1819 - val_loss: 58.2702 - val_mae: 6.1986\n",
      "Epoch 704/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31809.2090 - mae: 118.0082 - val_loss: 19699.3203 - val_mae: 123.9114\n",
      "Epoch 705/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33551.6641 - mae: 122.2189 - val_loss: 687.9199 - val_mae: 21.5720\n",
      "Epoch 706/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31223.9688 - mae: 103.8846 - val_loss: 72704.1719 - val_mae: 239.7138\n",
      "Epoch 707/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33046.9883 - mae: 105.4338 - val_loss: 404.6717 - val_mae: 17.9099\n",
      "Epoch 708/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34898.0117 - mae: 105.9111 - val_loss: 107252.4766 - val_mae: 290.7812\n",
      "Epoch 709/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31329.4922 - mae: 120.9149 - val_loss: 31793.5332 - val_mae: 156.8940\n",
      "Epoch 710/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35759.0742 - mae: 130.3892 - val_loss: 1303.5035 - val_mae: 33.3236\n",
      "Epoch 711/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34765.3438 - mae: 130.2315 - val_loss: 1222.2468 - val_mae: 29.3792\n",
      "Epoch 712/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33591.3086 - mae: 136.5536 - val_loss: 24235.6816 - val_mae: 135.9419\n",
      "Epoch 713/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32093.7988 - mae: 128.5434 - val_loss: 43.1745 - val_mae: 5.3443\n",
      "Epoch 714/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30313.3340 - mae: 75.5750 - val_loss: 84.3611 - val_mae: 7.2373\n",
      "Epoch 715/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32089.7676 - mae: 105.1520 - val_loss: 494.5828 - val_mae: 18.8394\n",
      "Epoch 716/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34471.1562 - mae: 115.2931 - val_loss: 8880.8301 - val_mae: 80.8063\n",
      "Epoch 717/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30481.6465 - mae: 103.0730 - val_loss: 218042.5469 - val_mae: 410.1646\n",
      "Epoch 718/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33929.3516 - mae: 121.6297 - val_loss: 39933.8477 - val_mae: 176.8051\n",
      "Epoch 719/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36467.5234 - mae: 115.3646 - val_loss: 3752.6353 - val_mae: 53.3353\n",
      "Epoch 720/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33207.3945 - mae: 129.6243 - val_loss: 35912.0820 - val_mae: 165.4812\n",
      "Epoch 721/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33210.5781 - mae: 119.8313 - val_loss: 33.6720 - val_mae: 4.1745\n",
      "Epoch 722/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35426.5977 - mae: 124.9499 - val_loss: 599.3626 - val_mae: 22.2244\n",
      "Epoch 723/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29195.3672 - mae: 87.1289 - val_loss: 364.6515 - val_mae: 17.5354\n",
      "Epoch 724/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 37614.5234 - mae: 129.1745 - val_loss: 4752.6978 - val_mae: 60.8691\n",
      "Epoch 725/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32344.8066 - mae: 81.7319 - val_loss: 193.0795 - val_mae: 11.3901\n",
      "Epoch 726/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32814.2500 - mae: 122.3690 - val_loss: 21871.4277 - val_mae: 129.8237\n",
      "Epoch 727/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33931.3633 - mae: 133.5448 - val_loss: 234.6504 - val_mae: 13.2062\n",
      "Epoch 728/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32231.3535 - mae: 128.5903 - val_loss: 3156.6025 - val_mae: 47.7553\n",
      "Epoch 729/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32887.6562 - mae: 121.2030 - val_loss: 165537.7812 - val_mae: 356.3491\n",
      "Epoch 730/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35006.8828 - mae: 119.4013 - val_loss: 552.3741 - val_mae: 21.4967\n",
      "Epoch 731/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33725.4648 - mae: 124.1833 - val_loss: 43093.5898 - val_mae: 183.0137\n",
      "Epoch 732/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35376.0547 - mae: 131.1727 - val_loss: 1263.0436 - val_mae: 32.1103\n",
      "Epoch 733/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33474.4336 - mae: 134.6897 - val_loss: 83091.0859 - val_mae: 253.3546\n",
      "Epoch 734/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32969.7695 - mae: 126.3472 - val_loss: 45279.2383 - val_mae: 186.8204\n",
      "Epoch 735/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35530.1367 - mae: 140.4753 - val_loss: 1901.1667 - val_mae: 38.9707\n",
      "Epoch 736/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33788.6172 - mae: 105.2662 - val_loss: 11.2625 - val_mae: 2.6990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 737/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31683.4102 - mae: 91.9661 - val_loss: 908.8346 - val_mae: 24.6673\n",
      "Epoch 738/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30567.6738 - mae: 111.2494 - val_loss: 87374.9922 - val_mae: 257.7208\n",
      "Epoch 739/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34483.6680 - mae: 119.7778 - val_loss: 152357.3906 - val_mae: 345.4838\n",
      "Epoch 740/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33332.3359 - mae: 90.9184 - val_loss: 92386.3828 - val_mae: 269.4095\n",
      "Epoch 741/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31246.9297 - mae: 96.9824 - val_loss: 54.1713 - val_mae: 6.7633\n",
      "Epoch 742/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32760.4629 - mae: 110.0939 - val_loss: 3711.1787 - val_mae: 52.7138\n",
      "Epoch 743/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32599.0234 - mae: 109.0527 - val_loss: 233186.7031 - val_mae: 426.7979\n",
      "Epoch 744/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34716.6250 - mae: 117.1244 - val_loss: 63120.1953 - val_mae: 220.1038\n",
      "Epoch 745/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32219.2871 - mae: 118.2538 - val_loss: 2703.3396 - val_mae: 46.0907\n",
      "Epoch 746/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32692.2227 - mae: 127.6452 - val_loss: 2418.8284 - val_mae: 45.1243\n",
      "Epoch 747/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34285.7969 - mae: 96.5781 - val_loss: 188782.9844 - val_mae: 378.2466\n",
      "Epoch 748/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30709.9727 - mae: 128.0926 - val_loss: 100210.8672 - val_mae: 276.7282\n",
      "Epoch 749/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33770.4766 - mae: 105.6611 - val_loss: 169021.0938 - val_mae: 359.3218\n",
      "Epoch 750/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32998.4219 - mae: 113.7334 - val_loss: 69285.7578 - val_mae: 232.2209\n",
      "Epoch 751/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33628.8438 - mae: 129.3280 - val_loss: 45.9035 - val_mae: 5.4907\n",
      "Epoch 752/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35196.0078 - mae: 131.2132 - val_loss: 8.8224 - val_mae: 2.2403\n",
      "Epoch 753/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33359.8516 - mae: 118.8788 - val_loss: 2702.2046 - val_mae: 47.5371\n",
      "Epoch 754/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35022.3672 - mae: 148.5849 - val_loss: 8158.1152 - val_mae: 80.4136\n",
      "Epoch 755/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30225.5156 - mae: 103.7333 - val_loss: 192262.8594 - val_mae: 387.6080\n",
      "Epoch 756/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35192.8203 - mae: 130.5288 - val_loss: 3201.3337 - val_mae: 52.3978\n",
      "Epoch 757/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34339.4102 - mae: 137.4424 - val_loss: 4145.3770 - val_mae: 57.0639\n",
      "Epoch 758/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34554.9648 - mae: 128.5722 - val_loss: 121.5152 - val_mae: 10.3538\n",
      "Epoch 759/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36707.6992 - mae: 121.6527 - val_loss: 23884.2969 - val_mae: 137.4448\n",
      "Epoch 760/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 29278.4531 - mae: 90.5481 - val_loss: 9500.2236 - val_mae: 87.8214\n",
      "Epoch 761/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28927.3262 - mae: 104.2046 - val_loss: 77284.5156 - val_mae: 242.3743\n",
      "Epoch 762/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34889.5859 - mae: 122.2590 - val_loss: 46976.4141 - val_mae: 192.7930\n",
      "Epoch 763/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31348.7188 - mae: 127.3414 - val_loss: 157.8020 - val_mae: 9.6603\n",
      "Epoch 764/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35309.8164 - mae: 132.1095 - val_loss: 31418.1074 - val_mae: 158.3625\n",
      "Epoch 765/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29846.6406 - mae: 89.1442 - val_loss: 6.3189 - val_mae: 2.0368\n",
      "Epoch 766/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34765.5898 - mae: 130.5540 - val_loss: 39979.0234 - val_mae: 178.5786\n",
      "Epoch 767/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35710.1602 - mae: 138.6871 - val_loss: 77693.6172 - val_mae: 245.6773\n",
      "Epoch 768/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 27142.5566 - mae: 90.5030 - val_loss: 71207.8281 - val_mae: 236.4273\n",
      "Epoch 769/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 38309.5977 - mae: 137.2772 - val_loss: 63990.1719 - val_mae: 225.8126\n",
      "Epoch 770/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31769.7695 - mae: 126.7651 - val_loss: 37948.3867 - val_mae: 175.3734\n",
      "Epoch 771/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32783.1094 - mae: 128.2190 - val_loss: 32427.3633 - val_mae: 157.9934\n",
      "Epoch 772/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36639.8828 - mae: 147.8920 - val_loss: 38191.5391 - val_mae: 172.0221\n",
      "Epoch 773/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34354.1484 - mae: 135.3048 - val_loss: 15994.7822 - val_mae: 113.0364\n",
      "Epoch 774/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31276.9395 - mae: 103.6203 - val_loss: 100153.3516 - val_mae: 276.9005\n",
      "Epoch 775/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32880.8555 - mae: 121.2736 - val_loss: 4759.9287 - val_mae: 61.3358\n",
      "Epoch 776/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33089.9570 - mae: 92.2932 - val_loss: 231540.2031 - val_mae: 423.8265\n",
      "Epoch 777/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35740.0898 - mae: 118.7837 - val_loss: 323.2016 - val_mae: 17.2239\n",
      "Epoch 778/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35435.7383 - mae: 135.5801 - val_loss: 373.4327 - val_mae: 17.3415\n",
      "Epoch 779/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31393.7832 - mae: 104.8393 - val_loss: 2504.0728 - val_mae: 42.7086\n",
      "Epoch 780/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32946.3008 - mae: 120.7820 - val_loss: 12209.3027 - val_mae: 97.6870\n",
      "Epoch 781/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34334.9922 - mae: 118.0505 - val_loss: 614.6432 - val_mae: 22.0894\n",
      "Epoch 782/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32372.1055 - mae: 110.2546 - val_loss: 102200.7578 - val_mae: 282.8800\n",
      "Epoch 783/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 38921.2070 - mae: 120.5699 - val_loss: 14892.7285 - val_mae: 105.9839\n",
      "Epoch 784/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 28729.2051 - mae: 103.7473 - val_loss: 67.5665 - val_mae: 6.6891\n",
      "Epoch 785/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32926.7461 - mae: 119.1079 - val_loss: 2541.3789 - val_mae: 44.9897\n",
      "Epoch 786/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32830.1055 - mae: 129.5202 - val_loss: 65440.9844 - val_mae: 227.1085\n",
      "Epoch 787/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33105.2383 - mae: 124.5655 - val_loss: 149579.9531 - val_mae: 342.1233\n",
      "Epoch 788/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32107.8770 - mae: 98.6160 - val_loss: 187861.0469 - val_mae: 383.0135\n",
      "Epoch 789/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35615.4414 - mae: 144.8805 - val_loss: 6399.5088 - val_mae: 72.7812\n",
      "Epoch 790/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 31564.0195 - mae: 137.8555 - val_loss: 18198.4648 - val_mae: 120.6996\n",
      "Epoch 791/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35433.2344 - mae: 132.3325 - val_loss: 395.6008 - val_mae: 18.9538\n",
      "Epoch 792/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30369.7695 - mae: 104.1111 - val_loss: 113163.9219 - val_mae: 297.6090\n",
      "Epoch 793/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35777.3867 - mae: 131.1402 - val_loss: 2799.6816 - val_mae: 49.2524\n",
      "Epoch 794/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34425.7695 - mae: 135.1171 - val_loss: 171.5992 - val_mae: 9.9859\n",
      "Epoch 795/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30296.2871 - mae: 103.3566 - val_loss: 210076.5156 - val_mae: 405.5078\n",
      "Epoch 796/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31972.7129 - mae: 90.0535 - val_loss: 61398.9922 - val_mae: 216.2763\n",
      "Epoch 797/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32005.7012 - mae: 121.8483 - val_loss: 280.3282 - val_mae: 15.0154\n",
      "Epoch 798/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34342.0898 - mae: 108.9210 - val_loss: 118355.7188 - val_mae: 299.2393\n",
      "Epoch 799/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32881.6562 - mae: 116.0182 - val_loss: 2264.2214 - val_mae: 42.5828\n",
      "Epoch 800/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34868.8477 - mae: 116.5798 - val_loss: 3701.4995 - val_mae: 51.9136\n",
      "Epoch 801/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31386.7598 - mae: 127.4576 - val_loss: 19294.5000 - val_mae: 120.4079\n",
      "Epoch 802/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35876.3789 - mae: 143.0563 - val_loss: 27.5857 - val_mae: 4.6355\n",
      "Epoch 803/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33619.1484 - mae: 131.8215 - val_loss: 158041.8906 - val_mae: 348.3537\n",
      "Epoch 804/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33025.6562 - mae: 106.9571 - val_loss: 24.2003 - val_mae: 3.8975\n",
      "Epoch 805/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34066.7227 - mae: 135.1536 - val_loss: 38881.3789 - val_mae: 174.2614\n",
      "Epoch 806/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32950.1406 - mae: 118.1276 - val_loss: 87093.9141 - val_mae: 261.3803\n",
      "Epoch 807/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34834.3477 - mae: 146.7182 - val_loss: 71804.6094 - val_mae: 233.8493\n",
      "Epoch 808/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32731.9551 - mae: 113.7438 - val_loss: 35.9349 - val_mae: 4.7629\n",
      "Epoch 809/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33669.2383 - mae: 112.3983 - val_loss: 34620.3711 - val_mae: 166.8694\n",
      "Epoch 810/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29441.9961 - mae: 91.3944 - val_loss: 9164.1738 - val_mae: 85.0629\n",
      "Epoch 811/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36777.0156 - mae: 134.1821 - val_loss: 5235.3403 - val_mae: 65.1167\n",
      "Epoch 812/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35183.3828 - mae: 138.3457 - val_loss: 22940.6055 - val_mae: 132.4690\n",
      "Epoch 813/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33983.8242 - mae: 102.0493 - val_loss: 111.6063 - val_mae: 10.0073\n",
      "Epoch 814/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32265.2578 - mae: 93.4316 - val_loss: 20.4341 - val_mae: 3.6454\n",
      "Epoch 815/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33906.7734 - mae: 117.3049 - val_loss: 58902.0742 - val_mae: 210.6700\n",
      "Epoch 816/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32268.3496 - mae: 102.6877 - val_loss: 45468.9805 - val_mae: 188.3760\n",
      "Epoch 817/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32992.6953 - mae: 122.7968 - val_loss: 232622.4219 - val_mae: 421.3306\n",
      "Epoch 818/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33988.6289 - mae: 112.6484 - val_loss: 1156.4296 - val_mae: 31.3449\n",
      "Epoch 819/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33747.4922 - mae: 130.2614 - val_loss: 273.0136 - val_mae: 15.1161\n",
      "Epoch 820/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30885.4941 - mae: 103.8968 - val_loss: 6294.6611 - val_mae: 67.3804\n",
      "Epoch 821/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36619.3516 - mae: 146.2819 - val_loss: 32938.6055 - val_mae: 158.3782\n",
      "Epoch 822/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32078.2441 - mae: 128.3566 - val_loss: 108294.7656 - val_mae: 289.1670\n",
      "Epoch 823/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34632.6953 - mae: 104.8303 - val_loss: 3469.6038 - val_mae: 55.2882\n",
      "Epoch 824/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30351.3809 - mae: 110.6610 - val_loss: 268657.4688 - val_mae: 459.7140\n",
      "Epoch 825/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30727.6016 - mae: 93.0285 - val_loss: 57020.6719 - val_mae: 208.5849\n",
      "Epoch 826/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35970.8398 - mae: 125.1254 - val_loss: 276.4081 - val_mae: 13.5422\n",
      "Epoch 827/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33200.6719 - mae: 103.6278 - val_loss: 141.0278 - val_mae: 10.3706\n",
      "Epoch 828/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32462.7969 - mae: 109.1849 - val_loss: 35.9250 - val_mae: 5.0267\n",
      "Epoch 829/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33866.5703 - mae: 103.5988 - val_loss: 13.9597 - val_mae: 2.9602\n",
      "Epoch 830/2000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 32462.8457 - mae: 129.5603 - val_loss: 1846.4774 - val_mae: 37.2821\n",
      "Epoch 831/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37080.6914 - mae: 128.8683 - val_loss: 4428.7295 - val_mae: 56.5422\n",
      "Epoch 832/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31090.0391 - mae: 88.1844 - val_loss: 165.1070 - val_mae: 9.9273\n",
      "Epoch 833/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31662.1094 - mae: 109.2236 - val_loss: 1417.8102 - val_mae: 30.2860\n",
      "Epoch 834/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35325.3984 - mae: 101.2653 - val_loss: 14.4858 - val_mae: 3.0760\n",
      "Epoch 835/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33154.7500 - mae: 107.5019 - val_loss: 2318.7505 - val_mae: 43.6586\n",
      "Epoch 836/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34633.6914 - mae: 143.5252 - val_loss: 41992.9258 - val_mae: 179.5423\n",
      "Epoch 837/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31120.6445 - mae: 103.3920 - val_loss: 8538.8760 - val_mae: 81.6555\n",
      "Epoch 838/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38021.7891 - mae: 141.3922 - val_loss: 777.5710 - val_mae: 25.8410\n",
      "Epoch 839/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29568.8066 - mae: 81.8754 - val_loss: 2092.3401 - val_mae: 37.8676\n",
      "Epoch 840/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33326.9336 - mae: 134.4652 - val_loss: 2973.7810 - val_mae: 47.3117\n",
      "Epoch 841/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33969.8242 - mae: 136.1174 - val_loss: 185.4072 - val_mae: 12.6284\n",
      "Epoch 842/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35009.3906 - mae: 127.2868 - val_loss: 10183.2256 - val_mae: 90.7956\n",
      "Epoch 843/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33417.1133 - mae: 124.4478 - val_loss: 89960.3516 - val_mae: 264.1985\n",
      "Epoch 844/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32357.0762 - mae: 136.8115 - val_loss: 24532.8086 - val_mae: 136.5783\n",
      "Epoch 845/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34602.2695 - mae: 114.9441 - val_loss: 69.4612 - val_mae: 7.6910\n",
      "Epoch 846/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33511.2148 - mae: 125.6282 - val_loss: 414.7674 - val_mae: 17.2702\n",
      "Epoch 847/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33072.6914 - mae: 120.5234 - val_loss: 26.8982 - val_mae: 4.1460\n",
      "Epoch 848/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31377.8809 - mae: 104.1545 - val_loss: 3592.9160 - val_mae: 54.4559\n",
      "Epoch 849/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35522.1562 - mae: 122.5022 - val_loss: 29737.9238 - val_mae: 152.4880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 850/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29612.3379 - mae: 111.3730 - val_loss: 158716.4531 - val_mae: 351.9675\n",
      "Epoch 851/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36834.8203 - mae: 129.2168 - val_loss: 312.9290 - val_mae: 14.6062\n",
      "Epoch 852/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33372.4648 - mae: 103.9939 - val_loss: 32.3708 - val_mae: 4.5430\n",
      "Epoch 853/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31037.6582 - mae: 77.7953 - val_loss: 189.2951 - val_mae: 12.3293\n",
      "Epoch 854/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32722.0234 - mae: 106.5350 - val_loss: 211.4913 - val_mae: 13.0682\n",
      "Epoch 855/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33643.5430 - mae: 123.4281 - val_loss: 50125.4258 - val_mae: 198.2230\n",
      "Epoch 856/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32173.1504 - mae: 107.3334 - val_loss: 14636.2754 - val_mae: 104.2837\n",
      "Epoch 857/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33265.2305 - mae: 140.4983 - val_loss: 23193.2285 - val_mae: 130.6769\n",
      "Epoch 858/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35944.4688 - mae: 139.1101 - val_loss: 39515.8516 - val_mae: 176.0603\n",
      "Epoch 859/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33561.1875 - mae: 110.3682 - val_loss: 1684.7095 - val_mae: 38.8276\n",
      "Epoch 860/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31033.5840 - mae: 110.0795 - val_loss: 3845.2559 - val_mae: 53.6704\n",
      "Epoch 861/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34249.2422 - mae: 119.0094 - val_loss: 123863.1094 - val_mae: 312.2236\n",
      "Epoch 862/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33793.1016 - mae: 125.0176 - val_loss: 23.4398 - val_mae: 3.8961\n",
      "Epoch 863/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30486.7168 - mae: 104.0984 - val_loss: 165.5417 - val_mae: 10.1416\n",
      "Epoch 864/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34392.1953 - mae: 99.4707 - val_loss: 3723.7502 - val_mae: 57.3433\n",
      "Epoch 865/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32383.8887 - mae: 126.3881 - val_loss: 563.5417 - val_mae: 22.5833\n",
      "Epoch 866/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34057.8750 - mae: 123.0168 - val_loss: 11.3571 - val_mae: 2.7346\n",
      "Epoch 867/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31783.7598 - mae: 127.6960 - val_loss: 193845.9219 - val_mae: 389.7507\n",
      "Epoch 868/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34557.1914 - mae: 134.9955 - val_loss: 106984.2188 - val_mae: 287.9544\n",
      "Epoch 869/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34205.2266 - mae: 107.4011 - val_loss: 6392.8203 - val_mae: 67.7756\n",
      "Epoch 870/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32048.3379 - mae: 99.6590 - val_loss: 87761.9141 - val_mae: 259.8931\n",
      "Epoch 871/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35347.2578 - mae: 133.9482 - val_loss: 66.8366 - val_mae: 7.1157\n",
      "Epoch 872/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33834.4688 - mae: 100.3721 - val_loss: 7892.8926 - val_mae: 77.9005\n",
      "Epoch 873/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32945.4023 - mae: 114.7780 - val_loss: 13770.6162 - val_mae: 103.2562\n",
      "Epoch 874/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37547.3398 - mae: 150.1833 - val_loss: 2584.2905 - val_mae: 47.0429\n",
      "Epoch 875/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31166.4082 - mae: 108.4123 - val_loss: 22977.9121 - val_mae: 132.7657\n",
      "Epoch 876/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35339.3281 - mae: 133.2840 - val_loss: 12964.5430 - val_mae: 99.9537\n",
      "Epoch 877/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30869.3711 - mae: 94.0052 - val_loss: 1084.4650 - val_mae: 29.8838\n",
      "Epoch 878/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36094.4492 - mae: 83.0748 - val_loss: 9.3740 - val_mae: 2.4343\n",
      "Epoch 879/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32709.6719 - mae: 115.3679 - val_loss: 65.8945 - val_mae: 7.3324\n",
      "Epoch 880/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32353.1660 - mae: 115.5000 - val_loss: 257473.4375 - val_mae: 448.1667\n",
      "Epoch 881/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33974.0430 - mae: 112.5024 - val_loss: 1493.7626 - val_mae: 31.8668\n",
      "Epoch 882/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31864.9688 - mae: 123.7336 - val_loss: 68023.5312 - val_mae: 230.2699\n",
      "Epoch 883/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33113.2852 - mae: 97.0235 - val_loss: 33.2946 - val_mae: 4.6364\n",
      "Epoch 884/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34654.0547 - mae: 105.0883 - val_loss: 74.4158 - val_mae: 6.5145\n",
      "Epoch 885/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34534.7383 - mae: 123.7738 - val_loss: 90.3091 - val_mae: 7.0677\n",
      "Epoch 886/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31629.4492 - mae: 88.6217 - val_loss: 774.8068 - val_mae: 25.6397\n",
      "Epoch 887/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32980.5078 - mae: 104.4747 - val_loss: 125432.1797 - val_mae: 310.7188\n",
      "Epoch 888/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32631.8281 - mae: 103.9697 - val_loss: 25439.1250 - val_mae: 142.6628\n",
      "Epoch 889/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33729.9883 - mae: 120.9760 - val_loss: 16803.2695 - val_mae: 113.7746\n",
      "Epoch 890/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30282.6797 - mae: 96.4455 - val_loss: 4343.6025 - val_mae: 57.5199\n",
      "Epoch 891/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34836.8672 - mae: 116.6856 - val_loss: 49545.2188 - val_mae: 199.4363\n",
      "Epoch 892/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32944.6875 - mae: 110.7622 - val_loss: 17.5370 - val_mae: 3.5194\n",
      "Epoch 893/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33195.9023 - mae: 124.7625 - val_loss: 2008.1317 - val_mae: 38.5760\n",
      "Epoch 894/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33185.5391 - mae: 136.5825 - val_loss: 4053.3044 - val_mae: 56.9141\n",
      "Epoch 895/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32843.0742 - mae: 113.8339 - val_loss: 2486.5911 - val_mae: 42.7541\n",
      "Epoch 896/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33216.8398 - mae: 104.1799 - val_loss: 13.6228 - val_mae: 2.9559\n",
      "Epoch 897/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35614.2695 - mae: 125.1815 - val_loss: 10305.4883 - val_mae: 88.5517\n",
      "Epoch 898/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31202.3359 - mae: 112.5031 - val_loss: 3146.9414 - val_mae: 48.4887\n",
      "Epoch 899/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35886.4062 - mae: 143.2989 - val_loss: 53753.3125 - val_mae: 205.3424\n",
      "Epoch 900/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34192.1758 - mae: 135.1992 - val_loss: 41966.6406 - val_mae: 177.1080\n",
      "Epoch 901/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32100.1855 - mae: 135.4807 - val_loss: 200662.2656 - val_mae: 393.9247\n",
      "Epoch 902/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33300.7070 - mae: 102.1885 - val_loss: 2698.2654 - val_mae: 46.0073\n",
      "Epoch 903/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34571.5352 - mae: 118.2206 - val_loss: 297.5037 - val_mae: 13.6723\n",
      "Epoch 904/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34312.8008 - mae: 124.1280 - val_loss: 21006.7188 - val_mae: 127.1966\n",
      "Epoch 905/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32547.0957 - mae: 128.2049 - val_loss: 5889.3423 - val_mae: 66.6297\n",
      "Epoch 906/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35345.3320 - mae: 128.1376 - val_loss: 7572.1885 - val_mae: 78.1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 907/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32322.2070 - mae: 143.2634 - val_loss: 198126.1719 - val_mae: 394.7960\n",
      "Epoch 908/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33337.0039 - mae: 117.8153 - val_loss: 1521.8196 - val_mae: 36.9288\n",
      "Epoch 909/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32911.8555 - mae: 110.5883 - val_loss: 302369.9375 - val_mae: 481.5585\n",
      "Epoch 910/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35528.7148 - mae: 109.2160 - val_loss: 586.0320 - val_mae: 20.1833\n",
      "Epoch 911/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34110.6367 - mae: 116.6102 - val_loss: 9136.3877 - val_mae: 79.5383\n",
      "Epoch 912/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30819.5879 - mae: 115.3051 - val_loss: 680.1718 - val_mae: 23.3640\n",
      "Epoch 913/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34289.1133 - mae: 110.9730 - val_loss: 13698.3252 - val_mae: 104.4268\n",
      "Epoch 914/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28662.1152 - mae: 111.8861 - val_loss: 58125.0195 - val_mae: 215.9501\n",
      "Epoch 915/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36214.4961 - mae: 148.2218 - val_loss: 10043.5186 - val_mae: 92.5546\n",
      "Epoch 916/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32534.3730 - mae: 122.2359 - val_loss: 3328.1179 - val_mae: 49.9666\n",
      "Epoch 917/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31658.9844 - mae: 110.6181 - val_loss: 57865.2695 - val_mae: 213.5310\n",
      "Epoch 918/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32589.7930 - mae: 105.0795 - val_loss: 21.6426 - val_mae: 3.5912\n",
      "Epoch 919/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32941.7461 - mae: 126.0470 - val_loss: 338340.2188 - val_mae: 511.5474\n",
      "Epoch 920/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36064.5938 - mae: 134.5391 - val_loss: 15176.8955 - val_mae: 111.1609\n",
      "Epoch 921/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30766.8281 - mae: 100.4866 - val_loss: 244031.5469 - val_mae: 434.8805\n",
      "Epoch 922/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32959.1641 - mae: 82.1459 - val_loss: 18483.8965 - val_mae: 121.5018\n",
      "Epoch 923/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32632.7012 - mae: 129.6266 - val_loss: 653.5601 - val_mae: 23.9967\n",
      "Epoch 924/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34098.6406 - mae: 104.4964 - val_loss: 59.9246 - val_mae: 6.7537\n",
      "Epoch 925/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35307.9727 - mae: 102.1799 - val_loss: 98423.8438 - val_mae: 275.1757\n",
      "Epoch 926/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31766.7793 - mae: 125.1366 - val_loss: 232.6931 - val_mae: 13.6733\n",
      "Epoch 927/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33768.5820 - mae: 147.7436 - val_loss: 96949.0703 - val_mae: 272.0978\n",
      "Epoch 928/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 38166.3359 - mae: 140.8334 - val_loss: 81.0939 - val_mae: 6.9367\n",
      "Epoch 929/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31509.5117 - mae: 126.7598 - val_loss: 16801.7363 - val_mae: 114.5831\n",
      "Epoch 930/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33075.8008 - mae: 117.2814 - val_loss: 1262.2283 - val_mae: 30.4972\n",
      "Epoch 931/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31880.2598 - mae: 113.2635 - val_loss: 6086.5991 - val_mae: 69.0839\n",
      "Epoch 932/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35725.8516 - mae: 135.6855 - val_loss: 102175.2344 - val_mae: 279.7916\n",
      "Epoch 933/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33171.3242 - mae: 124.3606 - val_loss: 5305.7949 - val_mae: 65.7652\n",
      "Epoch 934/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30865.0449 - mae: 103.4223 - val_loss: 94277.3984 - val_mae: 268.4346\n",
      "Epoch 935/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37055.1016 - mae: 131.6108 - val_loss: 41043.7109 - val_mae: 177.1952\n",
      "Epoch 936/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32000.1094 - mae: 99.2918 - val_loss: 546.2334 - val_mae: 21.6411\n",
      "Epoch 937/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33788.3789 - mae: 129.1924 - val_loss: 54165.2734 - val_mae: 207.6060\n",
      "Epoch 938/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32910.2617 - mae: 112.0233 - val_loss: 140.0804 - val_mae: 9.0890\n",
      "Epoch 939/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32136.0020 - mae: 118.5308 - val_loss: 2983.1392 - val_mae: 48.2103\n",
      "Epoch 940/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34126.0742 - mae: 138.6350 - val_loss: 24408.3438 - val_mae: 138.4427\n",
      "Epoch 941/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34754.2773 - mae: 114.6276 - val_loss: 5235.9048 - val_mae: 65.9847\n",
      "Epoch 942/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32112.0605 - mae: 118.2730 - val_loss: 35700.8672 - val_mae: 167.4092\n",
      "Epoch 943/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36747.0156 - mae: 137.7800 - val_loss: 3167.3489 - val_mae: 51.3767\n",
      "Epoch 944/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33655.8086 - mae: 135.0292 - val_loss: 4594.0381 - val_mae: 58.4850\n",
      "Epoch 945/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31746.1055 - mae: 113.8499 - val_loss: 117701.8281 - val_mae: 301.0659\n",
      "Epoch 946/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31786.2812 - mae: 112.4735 - val_loss: 45453.4102 - val_mae: 187.5678\n",
      "Epoch 947/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35141.3125 - mae: 136.5550 - val_loss: 701.2415 - val_mae: 23.8661\n",
      "Epoch 948/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33052.8789 - mae: 113.3681 - val_loss: 635.7052 - val_mae: 23.6866\n",
      "Epoch 949/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32929.9453 - mae: 113.1516 - val_loss: 65.7201 - val_mae: 6.5934\n",
      "Epoch 950/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32379.3730 - mae: 87.9769 - val_loss: 2895.8325 - val_mae: 47.4388\n",
      "Epoch 951/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36176.4297 - mae: 128.2637 - val_loss: 40096.2578 - val_mae: 177.6468\n",
      "Epoch 952/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30652.8496 - mae: 119.6806 - val_loss: 81231.1328 - val_mae: 254.3665\n",
      "Epoch 953/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33078.4219 - mae: 128.7459 - val_loss: 7395.2188 - val_mae: 76.8707\n",
      "Epoch 954/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32809.7344 - mae: 127.1905 - val_loss: 13682.4297 - val_mae: 104.4113\n",
      "Epoch 955/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33405.8477 - mae: 133.0721 - val_loss: 6816.1919 - val_mae: 75.8347\n",
      "Epoch 956/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34030.8047 - mae: 116.3537 - val_loss: 23642.6875 - val_mae: 135.1135\n",
      "Epoch 957/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32369.9102 - mae: 108.7581 - val_loss: 483.8218 - val_mae: 19.3304\n",
      "Epoch 958/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33874.4453 - mae: 130.0976 - val_loss: 12.2356 - val_mae: 2.7493\n",
      "Epoch 959/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34573.9219 - mae: 111.5240 - val_loss: 38245.6094 - val_mae: 166.5339\n",
      "Epoch 960/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32408.2773 - mae: 101.5567 - val_loss: 2838.2590 - val_mae: 49.0510\n",
      "Epoch 961/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31184.9043 - mae: 119.9526 - val_loss: 8287.1660 - val_mae: 84.7826\n",
      "Epoch 962/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35720.3633 - mae: 117.9324 - val_loss: 4163.4512 - val_mae: 56.6406\n",
      "Epoch 963/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30860.8594 - mae: 121.8250 - val_loss: 95024.5625 - val_mae: 270.1606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 964/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36890.7500 - mae: 137.3863 - val_loss: 27149.3379 - val_mae: 145.3115\n",
      "Epoch 965/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31456.2109 - mae: 135.9727 - val_loss: 215714.7969 - val_mae: 408.4410\n",
      "Epoch 966/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32773.1211 - mae: 129.6873 - val_loss: 15870.2324 - val_mae: 112.4770\n",
      "Epoch 967/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34114.3633 - mae: 119.6493 - val_loss: 27.6229 - val_mae: 4.3845\n",
      "Epoch 968/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35990.4922 - mae: 101.6132 - val_loss: 730.7966 - val_mae: 23.0030\n",
      "Epoch 969/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30629.5527 - mae: 119.9642 - val_loss: 61286.6211 - val_mae: 219.9173\n",
      "Epoch 970/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31635.8535 - mae: 110.9975 - val_loss: 309.8161 - val_mae: 15.1285\n",
      "Epoch 971/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32813.4219 - mae: 114.6796 - val_loss: 43740.7852 - val_mae: 183.1136\n",
      "Epoch 972/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32304.8281 - mae: 111.4303 - val_loss: 37087.8359 - val_mae: 169.8215\n",
      "Epoch 973/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36379.2500 - mae: 127.3222 - val_loss: 1433.1487 - val_mae: 30.8793\n",
      "Epoch 974/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32372.7324 - mae: 129.4723 - val_loss: 32.6096 - val_mae: 4.6435\n",
      "Epoch 975/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33126.4375 - mae: 121.0907 - val_loss: 81369.4531 - val_mae: 252.6104\n",
      "Epoch 976/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35164.8633 - mae: 126.4974 - val_loss: 6867.6089 - val_mae: 73.9220\n",
      "Epoch 977/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31660.6680 - mae: 122.0398 - val_loss: 81518.7500 - val_mae: 254.8492\n",
      "Epoch 978/2000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 33888.5859 - mae: 107.0183 - val_loss: 87.9624 - val_mae: 7.5017\n",
      "Epoch 979/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35717.2461 - mae: 88.5048 - val_loss: 74.8098 - val_mae: 7.8536\n",
      "Epoch 980/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34209.8203 - mae: 134.3663 - val_loss: 19863.5039 - val_mae: 124.2349\n",
      "Epoch 981/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35229.0664 - mae: 126.8592 - val_loss: 168.7601 - val_mae: 11.8591\n",
      "Epoch 982/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30431.9590 - mae: 80.0703 - val_loss: 226970.0938 - val_mae: 424.0815\n",
      "Epoch 983/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32335.8223 - mae: 124.9004 - val_loss: 24097.6602 - val_mae: 137.7853\n",
      "Epoch 984/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33112.8086 - mae: 102.5364 - val_loss: 141915.3594 - val_mae: 335.0337\n",
      "Epoch 985/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34437.3555 - mae: 115.5368 - val_loss: 388.4136 - val_mae: 16.0351\n",
      "Epoch 986/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33719.3047 - mae: 109.7058 - val_loss: 6037.3545 - val_mae: 68.4819\n",
      "Epoch 987/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34568.7188 - mae: 108.3494 - val_loss: 119.6542 - val_mae: 9.9601\n",
      "Epoch 988/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33556.8359 - mae: 121.1159 - val_loss: 8635.7754 - val_mae: 84.4808\n",
      "Epoch 989/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31656.6992 - mae: 110.7744 - val_loss: 42918.3789 - val_mae: 183.3515\n",
      "Epoch 990/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34444.4805 - mae: 99.8989 - val_loss: 71976.1641 - val_mae: 237.7160\n",
      "Epoch 991/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32191.0977 - mae: 106.2756 - val_loss: 43.7739 - val_mae: 5.3101\n",
      "Epoch 992/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33180.1289 - mae: 115.8198 - val_loss: 1221.2802 - val_mae: 32.2032\n",
      "Epoch 993/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37467.7891 - mae: 142.5832 - val_loss: 71.6292 - val_mae: 6.4179\n",
      "Epoch 994/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29006.5801 - mae: 87.5513 - val_loss: 80153.6328 - val_mae: 251.6846\n",
      "Epoch 995/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36667.5352 - mae: 128.4088 - val_loss: 9148.4580 - val_mae: 82.6884\n",
      "Epoch 996/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32620.5156 - mae: 125.2918 - val_loss: 102520.9531 - val_mae: 286.1211\n",
      "Epoch 997/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30971.6973 - mae: 112.7664 - val_loss: 59.3297 - val_mae: 5.5058\n",
      "Epoch 998/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32476.6504 - mae: 102.4626 - val_loss: 350.2252 - val_mae: 15.8319\n",
      "Epoch 999/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34450.3984 - mae: 107.7809 - val_loss: 27.1680 - val_mae: 4.5140\n",
      "Epoch 1000/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33192.3438 - mae: 116.1936 - val_loss: 4908.2432 - val_mae: 59.9258\n",
      "Epoch 1001/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33626.1484 - mae: 125.1145 - val_loss: 1211.9570 - val_mae: 31.7718\n",
      "Epoch 1002/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36921.5898 - mae: 111.0799 - val_loss: 183.4714 - val_mae: 9.7616\n",
      "Epoch 1003/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28627.7559 - mae: 91.3003 - val_loss: 3259.1777 - val_mae: 51.0581\n",
      "Epoch 1004/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37631.2734 - mae: 144.4292 - val_loss: 11372.0410 - val_mae: 95.2990\n",
      "Epoch 1005/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31856.0449 - mae: 121.1270 - val_loss: 7392.8706 - val_mae: 77.2767\n",
      "Epoch 1006/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31840.6152 - mae: 112.7060 - val_loss: 120339.5781 - val_mae: 307.2554\n",
      "Epoch 1007/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35516.7305 - mae: 106.6203 - val_loss: 37349.5547 - val_mae: 170.0844\n",
      "Epoch 1008/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32392.0996 - mae: 107.8899 - val_loss: 10319.8906 - val_mae: 88.7533\n",
      "Epoch 1009/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32092.3945 - mae: 132.6791 - val_loss: 4939.9751 - val_mae: 63.7762\n",
      "Epoch 1010/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36143.1172 - mae: 127.9466 - val_loss: 28501.2070 - val_mae: 148.2588\n",
      "Epoch 1011/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33330.3516 - mae: 107.6894 - val_loss: 98994.0000 - val_mae: 271.2181\n",
      "Epoch 1012/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31748.9629 - mae: 128.1359 - val_loss: 396.2458 - val_mae: 16.2299\n",
      "Epoch 1013/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32314.9316 - mae: 112.3495 - val_loss: 2899.1553 - val_mae: 46.5835\n",
      "Epoch 1014/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35630.1367 - mae: 110.9804 - val_loss: 1026.0887 - val_mae: 27.7099\n",
      "Epoch 1015/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30010.2129 - mae: 99.0912 - val_loss: 38184.0820 - val_mae: 172.5515\n",
      "Epoch 1016/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31477.1914 - mae: 107.2544 - val_loss: 90628.6641 - val_mae: 264.7686\n",
      "Epoch 1017/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36446.0664 - mae: 138.6001 - val_loss: 99959.0391 - val_mae: 277.1658\n",
      "Epoch 1018/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33270.8164 - mae: 120.9616 - val_loss: 112146.1641 - val_mae: 297.9859\n",
      "Epoch 1019/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34823.1328 - mae: 127.3504 - val_loss: 22371.7188 - val_mae: 132.3922\n",
      "Epoch 1020/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32490.1211 - mae: 117.8202 - val_loss: 29.3162 - val_mae: 4.2948\n",
      "Epoch 1021/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32535.4180 - mae: 116.3303 - val_loss: 185984.8125 - val_mae: 375.9783\n",
      "Epoch 1022/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32607.3418 - mae: 109.3344 - val_loss: 26449.4297 - val_mae: 144.8092\n",
      "Epoch 1023/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32472.1094 - mae: 116.5325 - val_loss: 457.0373 - val_mae: 17.7495\n",
      "Epoch 1024/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35771.0859 - mae: 124.9637 - val_loss: 77.7470 - val_mae: 7.1760\n",
      "Epoch 1025/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34253.5273 - mae: 104.5127 - val_loss: 32.0425 - val_mae: 4.5293\n",
      "Epoch 1026/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30905.3203 - mae: 98.7770 - val_loss: 48956.9492 - val_mae: 195.7939\n",
      "Epoch 1027/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35613.8945 - mae: 113.6807 - val_loss: 106.3304 - val_mae: 9.1615\n",
      "Epoch 1028/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30817.6484 - mae: 102.5284 - val_loss: 828.9092 - val_mae: 25.1461\n",
      "Epoch 1029/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36769.7656 - mae: 131.6875 - val_loss: 28.6575 - val_mae: 4.7566\n",
      "Epoch 1030/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31143.8047 - mae: 115.5219 - val_loss: 24295.1641 - val_mae: 137.0181\n",
      "Epoch 1031/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33696.5391 - mae: 135.1420 - val_loss: 3948.4792 - val_mae: 57.4157\n",
      "Epoch 1032/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34565.0508 - mae: 117.4899 - val_loss: 2266.5713 - val_mae: 41.1576\n",
      "Epoch 1033/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32486.3613 - mae: 122.1933 - val_loss: 162.9592 - val_mae: 10.6165\n",
      "Epoch 1034/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32862.4180 - mae: 103.9086 - val_loss: 255612.2656 - val_mae: 447.7468\n",
      "Epoch 1035/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34473.7812 - mae: 124.2613 - val_loss: 41345.5312 - val_mae: 179.3140\n",
      "Epoch 1036/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32022.4102 - mae: 114.1769 - val_loss: 259.2938 - val_mae: 13.4174\n",
      "Epoch 1037/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32999.6719 - mae: 117.7129 - val_loss: 22596.6074 - val_mae: 133.7565\n",
      "Epoch 1038/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35975.2578 - mae: 140.6703 - val_loss: 238.8043 - val_mae: 13.7179\n",
      "Epoch 1039/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31343.2988 - mae: 83.5858 - val_loss: 4984.9824 - val_mae: 62.3748\n",
      "Epoch 1040/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36127.4336 - mae: 128.0216 - val_loss: 5484.7573 - val_mae: 66.4334\n",
      "Epoch 1041/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35201.4766 - mae: 147.0643 - val_loss: 93432.6406 - val_mae: 270.3687\n",
      "Epoch 1042/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31830.0996 - mae: 111.7502 - val_loss: 28844.0273 - val_mae: 148.2494\n",
      "Epoch 1043/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32441.5312 - mae: 130.2852 - val_loss: 149.4366 - val_mae: 10.0555\n",
      "Epoch 1044/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33155.3555 - mae: 143.2790 - val_loss: 155999.8438 - val_mae: 349.6297\n",
      "Epoch 1045/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34461.8281 - mae: 140.7697 - val_loss: 227177.1406 - val_mae: 421.6775\n",
      "Epoch 1046/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36895.5898 - mae: 136.8338 - val_loss: 128.6519 - val_mae: 9.8740\n",
      "Epoch 1047/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29024.2539 - mae: 87.1314 - val_loss: 109.9666 - val_mae: 7.9952\n",
      "Epoch 1048/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34481.5664 - mae: 111.9961 - val_loss: 18971.0000 - val_mae: 122.5511\n",
      "Epoch 1049/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35405.6250 - mae: 128.5818 - val_loss: 31.9402 - val_mae: 4.4707\n",
      "Epoch 1050/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31974.0078 - mae: 109.5263 - val_loss: 42464.5664 - val_mae: 179.9141\n",
      "Epoch 1051/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31616.7305 - mae: 119.4106 - val_loss: 62186.3359 - val_mae: 221.4243\n",
      "Epoch 1052/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37125.7852 - mae: 150.7661 - val_loss: 161343.3750 - val_mae: 352.7259\n",
      "Epoch 1053/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31117.6660 - mae: 123.7582 - val_loss: 61250.6523 - val_mae: 216.0263\n",
      "Epoch 1054/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32807.6523 - mae: 107.0936 - val_loss: 41580.0742 - val_mae: 180.8333\n",
      "Epoch 1055/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33016.6055 - mae: 129.9078 - val_loss: 53316.4336 - val_mae: 204.3725\n",
      "Epoch 1056/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34091.3008 - mae: 113.0742 - val_loss: 12120.0762 - val_mae: 96.3248\n",
      "Epoch 1057/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36232.9492 - mae: 137.4942 - val_loss: 57.3553 - val_mae: 5.8213\n",
      "Epoch 1058/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29908.4785 - mae: 114.6385 - val_loss: 3470.6665 - val_mae: 51.7520\n",
      "Epoch 1059/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33490.6914 - mae: 108.2760 - val_loss: 958.5526 - val_mae: 28.1969\n",
      "Epoch 1060/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37307.9062 - mae: 129.8949 - val_loss: 5376.6616 - val_mae: 67.0268\n",
      "Epoch 1061/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31335.0918 - mae: 93.1610 - val_loss: 1005.3020 - val_mae: 29.9283\n",
      "Epoch 1062/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32598.5664 - mae: 125.6608 - val_loss: 2617.0037 - val_mae: 44.8020\n",
      "Epoch 1063/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34599.4414 - mae: 128.8583 - val_loss: 135349.0781 - val_mae: 325.9908\n",
      "Epoch 1064/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33853.3398 - mae: 96.6546 - val_loss: 1885.2836 - val_mae: 38.6783\n",
      "Epoch 1065/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32224.7441 - mae: 131.7586 - val_loss: 19732.4590 - val_mae: 124.1407\n",
      "Epoch 1066/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34192.5859 - mae: 109.5150 - val_loss: 6273.8120 - val_mae: 70.9199\n",
      "Epoch 1067/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32508.4531 - mae: 110.4474 - val_loss: 139590.9688 - val_mae: 329.3678\n",
      "Epoch 1068/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34672.3711 - mae: 144.3211 - val_loss: 11094.8779 - val_mae: 92.3989\n",
      "Epoch 1069/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32703.0781 - mae: 94.5761 - val_loss: 85.0719 - val_mae: 6.6084\n",
      "Epoch 1070/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32228.9219 - mae: 104.3804 - val_loss: 9186.6104 - val_mae: 86.5156\n",
      "Epoch 1071/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34806.8555 - mae: 135.3997 - val_loss: 327658.6875 - val_mae: 501.9617\n",
      "Epoch 1072/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33986.0781 - mae: 100.2513 - val_loss: 23828.9590 - val_mae: 136.5176\n",
      "Epoch 1073/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30818.0527 - mae: 103.5314 - val_loss: 8964.3467 - val_mae: 82.9643\n",
      "Epoch 1074/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35334.3984 - mae: 115.5845 - val_loss: 8667.2246 - val_mae: 83.0177\n",
      "Epoch 1075/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33864.9492 - mae: 109.5100 - val_loss: 167.2446 - val_mae: 10.2572\n",
      "Epoch 1076/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 2ms/step - loss: 32765.7949 - mae: 123.4049 - val_loss: 142225.0000 - val_mae: 332.6855\n",
      "Epoch 1077/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30838.9551 - mae: 99.3485 - val_loss: 15742.7969 - val_mae: 112.0885\n",
      "Epoch 1078/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32975.3711 - mae: 114.1339 - val_loss: 156559.9688 - val_mae: 349.9050\n",
      "Epoch 1079/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34657.6562 - mae: 133.2871 - val_loss: 7389.9995 - val_mae: 75.6478\n",
      "Epoch 1080/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34716.8633 - mae: 113.6737 - val_loss: 198.4160 - val_mae: 12.6340\n",
      "Epoch 1081/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34917.0078 - mae: 137.0876 - val_loss: 30072.2305 - val_mae: 152.0160\n",
      "Epoch 1082/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30998.3477 - mae: 124.2604 - val_loss: 33492.7461 - val_mae: 162.5043\n",
      "Epoch 1083/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33507.5000 - mae: 109.6478 - val_loss: 31.8325 - val_mae: 4.7898\n",
      "Epoch 1084/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36376.5078 - mae: 114.5662 - val_loss: 20.3731 - val_mae: 3.4334\n",
      "Epoch 1085/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32632.0625 - mae: 126.5345 - val_loss: 40602.1094 - val_mae: 177.5386\n",
      "Epoch 1086/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31380.3457 - mae: 107.0709 - val_loss: 17440.6504 - val_mae: 117.1633\n",
      "Epoch 1087/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37128.9688 - mae: 103.8562 - val_loss: 7422.6938 - val_mae: 73.8601\n",
      "Epoch 1088/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30296.1406 - mae: 121.9470 - val_loss: 13529.5430 - val_mae: 102.4578\n",
      "Epoch 1089/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35270.9570 - mae: 139.5625 - val_loss: 13353.6455 - val_mae: 102.6089\n",
      "Epoch 1090/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33618.5156 - mae: 117.1322 - val_loss: 731.0562 - val_mae: 23.4294\n",
      "Epoch 1091/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31393.3145 - mae: 108.6323 - val_loss: 409.8220 - val_mae: 19.0965\n",
      "Epoch 1092/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32525.8066 - mae: 126.3870 - val_loss: 33865.0469 - val_mae: 161.7649\n",
      "Epoch 1093/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34727.2266 - mae: 105.8733 - val_loss: 56.5672 - val_mae: 5.7036\n",
      "Epoch 1094/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31749.9707 - mae: 111.1825 - val_loss: 4956.0620 - val_mae: 64.1953\n",
      "Epoch 1095/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33920.5000 - mae: 122.1028 - val_loss: 21101.7070 - val_mae: 128.8885\n",
      "Epoch 1096/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32229.8809 - mae: 119.6062 - val_loss: 91072.2109 - val_mae: 268.2194\n",
      "Epoch 1097/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37570.0898 - mae: 140.2059 - val_loss: 43105.3594 - val_mae: 186.7177\n",
      "Epoch 1098/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30229.9961 - mae: 98.7675 - val_loss: 1873.8323 - val_mae: 40.4413\n",
      "Epoch 1099/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31089.7500 - mae: 100.1329 - val_loss: 26268.9570 - val_mae: 142.6727\n",
      "Epoch 1100/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34198.9258 - mae: 124.2955 - val_loss: 88127.7578 - val_mae: 261.6934\n",
      "Epoch 1101/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34832.3633 - mae: 140.8832 - val_loss: 72133.3281 - val_mae: 238.5626\n",
      "Epoch 1102/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32215.9023 - mae: 116.7504 - val_loss: 2933.4409 - val_mae: 47.4910\n",
      "Epoch 1103/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35301.2617 - mae: 130.1900 - val_loss: 2908.2852 - val_mae: 50.7533\n",
      "Epoch 1104/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31648.8594 - mae: 112.1661 - val_loss: 13.9490 - val_mae: 2.8381\n",
      "Epoch 1105/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33480.5859 - mae: 114.3679 - val_loss: 30258.5566 - val_mae: 150.4646\n",
      "Epoch 1106/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33369.8320 - mae: 133.6327 - val_loss: 561.9052 - val_mae: 20.1244\n",
      "Epoch 1107/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34986.1055 - mae: 118.5130 - val_loss: 78.3328 - val_mae: 6.8568\n",
      "Epoch 1108/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29933.9531 - mae: 109.8078 - val_loss: 39719.5508 - val_mae: 173.8223\n",
      "Epoch 1109/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34925.2812 - mae: 129.8602 - val_loss: 10.0197 - val_mae: 2.4431\n",
      "Epoch 1110/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33018.2266 - mae: 121.2798 - val_loss: 13526.8311 - val_mae: 102.9584\n",
      "Epoch 1111/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35396.9609 - mae: 134.2055 - val_loss: 2295.8237 - val_mae: 41.2046\n",
      "Epoch 1112/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32828.2422 - mae: 130.2188 - val_loss: 15160.1777 - val_mae: 107.2670\n",
      "Epoch 1113/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35522.0078 - mae: 139.7215 - val_loss: 24093.4062 - val_mae: 136.3784\n",
      "Epoch 1114/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33720.2734 - mae: 93.8068 - val_loss: 6655.0449 - val_mae: 72.0162\n",
      "Epoch 1115/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29241.3047 - mae: 108.3842 - val_loss: 84.7742 - val_mae: 7.6589\n",
      "Epoch 1116/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32844.7266 - mae: 96.5750 - val_loss: 11430.9668 - val_mae: 92.0259\n",
      "Epoch 1117/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32842.8359 - mae: 105.2170 - val_loss: 20023.7422 - val_mae: 125.6492\n",
      "Epoch 1118/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36062.6758 - mae: 131.3916 - val_loss: 509.8452 - val_mae: 19.7053\n",
      "Epoch 1119/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28537.7988 - mae: 108.3162 - val_loss: 44591.1406 - val_mae: 187.3750\n",
      "Epoch 1120/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35442.9023 - mae: 107.1819 - val_loss: 18094.2715 - val_mae: 116.9523\n",
      "Epoch 1121/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 35788.6758 - mae: 133.1171 - val_loss: 10876.5576 - val_mae: 89.5913\n",
      "Epoch 1122/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35378.0508 - mae: 104.9130 - val_loss: 151.5110 - val_mae: 9.0484\n",
      "Epoch 1123/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29428.3574 - mae: 96.8758 - val_loss: 5863.4443 - val_mae: 69.0447\n",
      "Epoch 1124/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33932.2852 - mae: 125.3156 - val_loss: 11882.9795 - val_mae: 98.1648\n",
      "Epoch 1125/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36112.3047 - mae: 115.4365 - val_loss: 33.4652 - val_mae: 4.6738\n",
      "Epoch 1126/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 29893.7559 - mae: 88.2446 - val_loss: 16138.5840 - val_mae: 115.9192\n",
      "Epoch 1127/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34387.5820 - mae: 133.8508 - val_loss: 193123.8438 - val_mae: 385.1477\n",
      "Epoch 1128/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34350.6953 - mae: 98.1579 - val_loss: 188.5127 - val_mae: 12.6544\n",
      "Epoch 1129/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32145.5098 - mae: 126.6898 - val_loss: 160581.6719 - val_mae: 352.5749\n",
      "Epoch 1130/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35860.3359 - mae: 131.4537 - val_loss: 3633.2656 - val_mae: 50.5197\n",
      "Epoch 1131/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33315.6484 - mae: 121.1106 - val_loss: 10242.8184 - val_mae: 89.5288\n",
      "Epoch 1132/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31628.0508 - mae: 109.6780 - val_loss: 5128.2344 - val_mae: 65.8666\n",
      "Epoch 1133/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31631.6074 - mae: 95.5157 - val_loss: 2655.5896 - val_mae: 47.4057\n",
      "Epoch 1134/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 29954.4746 - mae: 98.5830 - val_loss: 91729.1953 - val_mae: 266.2081\n",
      "Epoch 1135/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34784.7617 - mae: 101.7307 - val_loss: 90067.4844 - val_mae: 265.1990\n",
      "Epoch 1136/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32665.5488 - mae: 97.1555 - val_loss: 201.1195 - val_mae: 11.8315\n",
      "Epoch 1137/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32127.6797 - mae: 107.1805 - val_loss: 141.4347 - val_mae: 10.4053\n",
      "Epoch 1138/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33476.4609 - mae: 113.3271 - val_loss: 147851.0000 - val_mae: 340.6005\n",
      "Epoch 1139/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36165.8633 - mae: 127.6438 - val_loss: 3918.8513 - val_mae: 54.9342\n",
      "Epoch 1140/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30679.3496 - mae: 126.6847 - val_loss: 8848.6475 - val_mae: 81.4489\n",
      "Epoch 1141/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34823.8438 - mae: 134.2608 - val_loss: 41281.5781 - val_mae: 175.8532\n",
      "Epoch 1142/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33410.6211 - mae: 119.8163 - val_loss: 43653.1797 - val_mae: 185.1915\n",
      "Epoch 1143/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32251.5391 - mae: 120.8508 - val_loss: 36719.2148 - val_mae: 170.3696\n",
      "Epoch 1144/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35559.6641 - mae: 126.7093 - val_loss: 38369.0898 - val_mae: 174.4343\n",
      "Epoch 1145/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33741.3906 - mae: 98.8487 - val_loss: 78.7863 - val_mae: 6.9402\n",
      "Epoch 1146/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31502.4941 - mae: 105.7827 - val_loss: 28606.4668 - val_mae: 153.1262\n",
      "Epoch 1147/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35480.0859 - mae: 109.3655 - val_loss: 74.6405 - val_mae: 7.5693\n",
      "Epoch 1148/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35932.8750 - mae: 107.0360 - val_loss: 3044.4082 - val_mae: 46.9771\n",
      "Epoch 1149/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31466.7773 - mae: 115.2402 - val_loss: 116394.5703 - val_mae: 301.9017\n",
      "Epoch 1150/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31593.4395 - mae: 125.6548 - val_loss: 3793.0051 - val_mae: 54.9587\n",
      "Epoch 1151/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32667.9062 - mae: 87.5941 - val_loss: 355.4281 - val_mae: 16.8558\n",
      "Epoch 1152/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33733.7695 - mae: 130.9350 - val_loss: 56815.2734 - val_mae: 213.2801\n",
      "Epoch 1153/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34207.1250 - mae: 126.1448 - val_loss: 36041.4102 - val_mae: 165.2185\n",
      "Epoch 1154/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33505.3789 - mae: 126.3002 - val_loss: 47.4153 - val_mae: 5.4650\n",
      "Epoch 1155/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32981.0742 - mae: 125.5266 - val_loss: 1505.1901 - val_mae: 33.8460\n",
      "Epoch 1156/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 35476.9961 - mae: 132.2475 - val_loss: 197.5797 - val_mae: 10.7898\n",
      "Epoch 1157/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30755.5645 - mae: 101.1749 - val_loss: 23574.5098 - val_mae: 136.4792\n",
      "Epoch 1158/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 36122.1367 - mae: 116.5049 - val_loss: 31130.0059 - val_mae: 156.9164\n",
      "Epoch 1159/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31902.9863 - mae: 108.1334 - val_loss: 66662.4922 - val_mae: 230.9123\n",
      "Epoch 1160/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32750.6602 - mae: 110.4993 - val_loss: 23.3323 - val_mae: 3.7311\n",
      "Epoch 1161/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32019.2461 - mae: 113.2573 - val_loss: 383.2966 - val_mae: 16.5405\n",
      "Epoch 1162/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34721.6875 - mae: 135.0914 - val_loss: 103553.0625 - val_mae: 281.6309\n",
      "Epoch 1163/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34208.5234 - mae: 131.1296 - val_loss: 75553.5859 - val_mae: 244.6497\n",
      "Epoch 1164/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34190.0977 - mae: 124.5452 - val_loss: 54.3870 - val_mae: 5.6059\n",
      "Epoch 1165/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32331.9004 - mae: 124.9425 - val_loss: 1853.7970 - val_mae: 40.7139\n",
      "Epoch 1166/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35278.3203 - mae: 132.8882 - val_loss: 107.4385 - val_mae: 6.8649\n",
      "Epoch 1167/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32934.5664 - mae: 123.4332 - val_loss: 758.0073 - val_mae: 25.4607\n",
      "Epoch 1168/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37298.3906 - mae: 140.4190 - val_loss: 117.6474 - val_mae: 8.7765\n",
      "Epoch 1169/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31946.3965 - mae: 125.2464 - val_loss: 33.4179 - val_mae: 4.5604\n",
      "Epoch 1170/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30851.5508 - mae: 103.5712 - val_loss: 611.9180 - val_mae: 22.0802\n",
      "Epoch 1171/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 36631.1172 - mae: 131.5554 - val_loss: 18876.9590 - val_mae: 124.8541\n",
      "Epoch 1172/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 28733.9902 - mae: 94.9332 - val_loss: 585.3743 - val_mae: 20.4562\n",
      "Epoch 1173/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 33838.4570 - mae: 125.0441 - val_loss: 2754.9624 - val_mae: 48.3768\n",
      "Epoch 1174/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32329.9648 - mae: 109.4659 - val_loss: 27298.2676 - val_mae: 146.2991\n",
      "Epoch 1175/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35731.4961 - mae: 127.7843 - val_loss: 3428.6802 - val_mae: 48.8918\n",
      "Epoch 1176/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35180.3867 - mae: 116.4403 - val_loss: 2635.6248 - val_mae: 44.4363\n",
      "Epoch 1177/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31624.7129 - mae: 100.8042 - val_loss: 185.6158 - val_mae: 12.3930\n",
      "Epoch 1178/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33318.0430 - mae: 126.4727 - val_loss: 6199.8042 - val_mae: 67.6989\n",
      "Epoch 1179/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31904.6699 - mae: 112.6567 - val_loss: 164172.9219 - val_mae: 357.7780\n",
      "Epoch 1180/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 34051.0078 - mae: 87.7664 - val_loss: 33249.2773 - val_mae: 160.8792\n",
      "Epoch 1181/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31983.0176 - mae: 133.4531 - val_loss: 1440.6417 - val_mae: 32.3110\n",
      "Epoch 1182/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 35005.3906 - mae: 119.9095 - val_loss: 483.0096 - val_mae: 17.2329\n",
      "Epoch 1183/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30521.6914 - mae: 81.2926 - val_loss: 20902.7656 - val_mae: 129.0233\n",
      "Epoch 1184/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33947.8516 - mae: 131.3783 - val_loss: 94937.4766 - val_mae: 270.1930\n",
      "Epoch 1185/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33777.8711 - mae: 135.5736 - val_loss: 24268.5469 - val_mae: 137.5454\n",
      "Epoch 1186/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34808.0156 - mae: 137.1438 - val_loss: 5461.5952 - val_mae: 63.0521\n",
      "Epoch 1187/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 31551.8945 - mae: 114.3571 - val_loss: 1043.4249 - val_mae: 30.0498\n",
      "Epoch 1188/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 1ms/step - loss: 33910.7812 - mae: 124.2590 - val_loss: 34869.3008 - val_mae: 163.4205\n",
      "Epoch 1189/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32459.3594 - mae: 127.8367 - val_loss: 25514.3770 - val_mae: 140.1461\n",
      "Epoch 1190/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34297.0977 - mae: 131.7952 - val_loss: 126758.1562 - val_mae: 314.1765\n",
      "Epoch 1191/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33361.1719 - mae: 133.3392 - val_loss: 65063.2148 - val_mae: 223.7568\n",
      "Epoch 1192/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37004.6289 - mae: 117.7198 - val_loss: 83.7829 - val_mae: 7.7841\n",
      "Epoch 1193/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 29060.3242 - mae: 90.1111 - val_loss: 112.3173 - val_mae: 8.6836\n",
      "Epoch 1194/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31563.1836 - mae: 95.6427 - val_loss: 131516.5938 - val_mae: 317.5553\n",
      "Epoch 1195/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37263.2500 - mae: 138.1252 - val_loss: 8390.7773 - val_mae: 83.5514\n",
      "Epoch 1196/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 30060.6328 - mae: 111.0860 - val_loss: 69.2007 - val_mae: 7.5065\n",
      "Epoch 1197/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33385.6641 - mae: 119.6393 - val_loss: 87963.7188 - val_mae: 263.6928\n",
      "Epoch 1198/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 34113.6758 - mae: 117.6747 - val_loss: 6713.6763 - val_mae: 72.1379\n",
      "Epoch 1199/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33262.3477 - mae: 103.4981 - val_loss: 56876.4062 - val_mae: 211.4431\n",
      "Epoch 1200/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31970.8184 - mae: 106.3959 - val_loss: 6599.0605 - val_mae: 71.7699\n",
      "Epoch 1201/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36809.0703 - mae: 116.8961 - val_loss: 1923.9910 - val_mae: 38.0313\n",
      "Epoch 1202/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30884.0859 - mae: 106.9259 - val_loss: 315451.9688 - val_mae: 491.4352\n",
      "Epoch 1203/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33763.6914 - mae: 118.0144 - val_loss: 109.3990 - val_mae: 8.5365\n",
      "Epoch 1204/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30942.1797 - mae: 109.5921 - val_loss: 1047.7540 - val_mae: 28.3819\n",
      "Epoch 1205/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 33185.5508 - mae: 119.0182 - val_loss: 43910.0820 - val_mae: 182.8000\n",
      "Epoch 1206/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32916.6719 - mae: 96.2333 - val_loss: 37683.0234 - val_mae: 173.5248\n",
      "Epoch 1207/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38103.7227 - mae: 131.7871 - val_loss: 44801.5781 - val_mae: 188.8207\n",
      "Epoch 1208/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31418.1797 - mae: 133.9809 - val_loss: 6816.8164 - val_mae: 72.8766\n",
      "Epoch 1209/2000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 32761.6172 - mae: 134.2167 - val_loss: 101704.2578 - val_mae: 281.7628\n",
      "Epoch 1210/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34829.3828 - mae: 108.7019 - val_loss: 695.7357 - val_mae: 22.3527\n",
      "Epoch 1211/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 35345.9727 - mae: 127.2450 - val_loss: 214.4804 - val_mae: 12.1445\n",
      "Epoch 1212/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32163.0469 - mae: 104.3670 - val_loss: 8310.5566 - val_mae: 82.1155\n",
      "Epoch 1213/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32651.4043 - mae: 110.0235 - val_loss: 50071.2227 - val_mae: 195.1470\n",
      "Epoch 1214/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34626.5469 - mae: 126.4664 - val_loss: 157037.2656 - val_mae: 351.4066\n",
      "Epoch 1215/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33759.7344 - mae: 102.7927 - val_loss: 8094.4780 - val_mae: 80.1165\n",
      "Epoch 1216/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33443.6641 - mae: 109.0322 - val_loss: 14878.0117 - val_mae: 107.3210\n",
      "Epoch 1217/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34502.3203 - mae: 116.7135 - val_loss: 43.8053 - val_mae: 5.4772\n",
      "Epoch 1218/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31227.9746 - mae: 108.9223 - val_loss: 14987.8330 - val_mae: 109.4769\n",
      "Epoch 1219/2000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 36606.0625 - mae: 120.8684 - val_loss: 58.2332 - val_mae: 6.0129\n",
      "Epoch 1220/2000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31032.0059 - mae: 105.7288 - val_loss: 228269.9062 - val_mae: 422.7722\n",
      "Epoch 1221/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 30509.0625 - mae: 108.6104 - val_loss: 26151.7461 - val_mae: 143.8117\n",
      "Epoch 1222/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 32980.5391 - mae: 134.9377 - val_loss: 91751.2812 - val_mae: 267.3558\n",
      "Epoch 1223/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 37318.1992 - mae: 108.8102 - val_loss: 14.8504 - val_mae: 3.1159\n",
      "Epoch 1224/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 31120.4805 - mae: 90.8850 - val_loss: 61282.4180 - val_mae: 219.1494\n",
      "Epoch 1225/2000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 38531.4922 - mae: 148.6521 - val_loss: 73.7557 - val_mae: 6.8118\n",
      "Epoch 1226/2000\n",
      "26/88 [=======>......................] - ETA: 0s - loss: 28504.0332 - mae: 71.0712"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-d79b33104955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    606\u001b[0m               cancellation_manager=cancellation_manager)\n\u001b[1;32m    607\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# TODO(akshayka): Either remove this if the FunctionLibraryRuntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=2000,batch_size=32,verbose=1,validation_data=(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278.7348983681649"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RfmInclusive[\"RFMScore\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
